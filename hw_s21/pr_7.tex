
% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------

\documentclass[10pt]{article}

\usepackage[margin=.7in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{enumitem}
\usepackage{tikz-cd}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{algorithm2e}
\usepackage{verse,stmaryrd}
\usepackage{fancyvrb}

% Number systems
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\PP}{\mathbb P}
\newcommand{\FF}{\mathbb F}
\newcommand{\DD}{\mathbb D}
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\ch}{\operatorname{ch}}
\newcommand{\Con}{\operatorname{Con}}
\newcommand{\coker}{\operatorname{coker}}
\newcommand{\CVect}{\CC\operatorname{-Vect}}
\newcommand{\Cantor}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\card}{\operatorname{card}}
\newcommand{\dbar}{\overline \partial}
\newcommand{\diam}{\operatorname{diam}}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\End}{\operatorname{End}}
\DeclareMathOperator*{\esssup}{ess\,sup}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\Ind}{\operatorname{Ind}}
\newcommand{\Inn}{\operatorname{Inn}}
\newcommand{\interior}{\operatorname{int}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\mesh}{\operatorname{mesh}}
\newcommand{\LL}{\mathcal L_0}
\newcommand{\Leb}{\mathcal{L}_{\text{loc}}^2}
\newcommand{\Lip}{\operatorname{Lip}}
\newcommand{\ppGL}{\operatorname{PGL}}
\newcommand{\ppic}{\vspace{35mm}}
\newcommand{\ppset}{\mathcal{P}}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator*{\Res}{Res}
\newcommand{\Riem}{\mathcal{R}}
\newcommand{\RVect}{\RR\operatorname{-Vect}}
\newcommand{\Sch}{\mathcal{S}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\spn}{\operatorname{span}}
\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\TT}{\mathcal T}
\DeclareMathOperator{\tr}{tr}

% Calculus of variations
\DeclareMathOperator{\pp}{\mathbf p}
\DeclareMathOperator{\zz}{\mathbf z}
\DeclareMathOperator{\uu}{\mathbf u}
\DeclareMathOperator{\vv}{\mathbf v}
\DeclareMathOperator{\ww}{\mathbf w}

% Categories
\newcommand{\Ab}{\mathbf{Ab}}
\newcommand{\Cat}{\mathbf{Cat}}
\newcommand{\Group}{\mathbf{Group}}
\newcommand{\Module}{\mathbf{Module}}
\newcommand{\Set}{\mathbf{Set}}
\DeclareMathOperator{\Fun}{Fun}
\DeclareMathOperator{\Iso}{Iso}

% Complex analysis
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Logic
\renewcommand{\iff}{\leftrightarrow}
\DeclareMathOperator{\Cov}{Cov}
\newcommand{\Henkin}{\operatorname{Henk}}
\newcommand{\PA}{\mathbf{PA}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\proves}{\vdash}

% Group
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Out}{Out}

% Other symbols
\newcommand{\heart}{\ensuremath\heartsuit}

\DeclareMathOperator{\atanh}{atanh}

% Theorems
\theoremstyle{definition}
\newtheorem*{corollary}{Corollary}
\newtheorem*{falselemma}{Grader's ``Lemma"}
\newtheorem{exer}{Exercise}
\newtheorem{lemma}{Lemma}[exer]
\newtheorem{theorem}[lemma]{Theorem}


\usepackage[backend=bibtex,style=alphabetic,maxcitenames=50,maxnames=50]{biblatex}
\renewbibmacro{in:}{}
\DeclareFieldFormat{pages}{#1}

\begin{document}
\noindent
\large\textbf{Probability II, HW 7} \hfill \textbf{Aidan Backus} \\
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------\

\begin{exer}
Let $0 < s < t$. Show that the probability of a Brownian motion having no zeroes on $[s, t]$ is $2\arcsin(\sqrt{s/t})/\pi$.
\end{exer}

Let $B$ be a Brownian motion. One has
\begin{equation}
\label{Exer 1 beginning}
\PP(\forall r \in [s, t]: B_r \neq 0) = 2\PP(\forall r \in [s, t]: B_r > 0).
\end{equation}
Let $\tilde B_r = B_{s + r} - B_s$.
Then $\tilde B$ is a Brownian motion and is independent of $\mathcal F_s$.
Since
$$\PP(\forall r \in [s, t]: B_r > 0|B_s \leq 0) = 0,$$
we restrict to the case $B_s = x > 0$; then the reflection theorem gives
\begin{align*}
\PP(\forall r \in [s, t]: B_r > 0|B_s = x) &= \PP(\forall r \in [s, t]: \tilde B_r + B_s > 0|B_s = x)\\
&= \PP(|\tilde B_{t - s}| < x|B_s = x)\\
&= 1 - 2F\left(\frac{x}{\sqrt{t - s}}\right)
\end{align*}
where $F$ is the standard normal cdf.
Plugging into (\ref{Exer 1 beginning}), we get
$$\PP(\forall r \in [s, t]: B_r \neq 0) = 2 - 4E\left[F\left(\frac{B_s}{\sqrt{t - s}}\right)1_{B_s > 0}\right].$$
We then compute
\begin{align*}
E\left[F\left(\frac{B_s}{\sqrt{t - s}}\right)1_{B_s > 0}\right] &= \int_0^\infty F\left(\frac{x}{\sqrt{t - s}}\right) ~d\mu_s(x)\\
&= \int_0^\infty F\left(\sqrt{\frac{s}{t - s}}x\right) ~dF(x)
\end{align*}
where $\mu_s$ is the distribution of $B_s$.
Now given $\alpha > 0$ we compute
\begin{align*}
\partial_\alpha \int_0^\infty F(\alpha x) ~dF(x) &= \int_0^\infty x F'(\alpha x) F'(x) ~dx\\
&= \frac{1}{2\pi} \int_0^\infty x\exp\left(-\frac{(1 + \alpha^2)x^2}{2}\right) ~dx\\
&= \frac{1}{2\pi(1 + \alpha^2)}.
\end{align*}
Applying the fundamental theorem with
$$\int_0^\infty F(\alpha x) ~dF(x)\bigg|_{\alpha = 0} = F(0) \PP(N(0, 1) > 1/2) = F(0)^2 = \frac{1}{4},$$
we get
$$E\left[F\left(\frac{B_s}{\sqrt{t - s}}\right)1_{B_s > 0}\right] = \frac{1}{4} + \frac{1}{2\pi} \arctan \sqrt{\frac{s}{t - s}}.$$
This gives
$$\PP(\forall r \in [s, t]: B_r \neq 0) = - \frac{2}{\pi} \arctan \sqrt{\frac{s}{t - s}} = \frac{2}{\pi} \arcsin \sqrt{\frac{s}{t}}.$$


\begin{exer}
Let $B$ be a Brownian motion. Let $\tau \in L^1$ be a stopping time. Show $EB_\tau = 0$ and $EB_\tau^2 = E\tau$.
\end{exer}

Let us proceed as in the usual application of optional stopping in discrete time, and consider the random variable
$$X = \sum_{s=0}^\infty |B_{s + 1} - B_s|1_{\tau > s}.$$
By monotone convergence,
$$EX = \sum_{s=0}^\infty E(|B_{s + 1} - B_s|1_{\tau > s}).$$
As $B$ is adapted by definition, and $B_{s + 1} - B_s$ is independent of $\mathcal F_s$ and identically distributed to $B_1$, while $1_{\tau > s}$ is determined by $\mathcal F_s$,
$$EX = E|B_1| \sum_{s=0}^\infty \PP(\tau > s) = E|B_1| \cdot E\tau < \infty$$
since $B_1 \sim N(0, 1)$ is in $L^1$.
So by dominated convergence (dominated by $X \in L^1$), the stopped Brownian motion $B^\tau$ satisfies
$$EB_\tau = \lim_{t \to \infty} EB^\tau_t = \lim_{t \to \infty} EB^\tau_0 = 0,$$
since, by optional stopping, stopped Brownian motions are martingales.

Now consider the adapted process $M_t = B_{t \wedge \tau}^2 - t \wedge \tau$.
If $t > s$ then, as stopped Brownian motions are martingales,
$$\Var(B_{t \wedge \tau}^2|\mathcal F_s) = (t - s) \wedge \tau$$
and hence
$$E(M_{\tau \wedge t}^2|\mathcal F_s) - t \wedge \tau = B_{\tau \wedge s}^2 - s \wedge \tau + \Var(B_{t \wedge \tau}^2|\mathcal F_s) - (t - s) \wedge \tau = M_s.$$
Therefore $M$ is a martingale, and so for every $t \geq 0$,
$$EM_t = EM_0 = EB_0^2 = 0.$$
It follows that
$$EB_{t \wedge \tau}^2 = E(t \wedge \tau) = \int_0^t \PP(\tau > s) ~ds.$$
One has
$$\lim_{t \to \infty} EB_{t \wedge \tau}^2 = \int_0^\infty \PP(\tau > s) ~ds = E\tau,$$
since $\tau \in L^1$.
On the other hand, $B_{t \wedge \tau} \to B_\tau$ almost surely, hence in $L^2$ by dominated convergence (where we replace the random variable $X$ above by
$$X = \sum_{s = 0}^\infty |B_{s + 1}^2 - B_s|1_{\tau > s}$$
and repeat the argument).
This implies that $\Var B_\tau = E\tau$.


\begin{exer}
Let $B$ be a Brownian motion and $\theta$ be a measurable adapted uniformly bounded process. Define
$$X_t = B_t + \int_0^t \theta_s ~ds$$
and
$$\log M_t = -\int_0^t \theta_s ~dB_s - \frac{1}{2} \int_0^t \theta_s^2 ~ds.$$
Show that $XM$ is a martingale. You can use the fact that if $u$ is a measurable adapted uniformly bounded process and
$$\log N_t = \int_0^t u_s ~dB_s - \frac{1}{2} \int_0^t u_s^2 ~ds$$
then $N$ is a martingale.
\end{exer}

By definition we have
$$dX_t = dB_t + \theta_t ~dt,$$
and we showed in class as part of the proof of Novikov's theorem that
$$dM_t + \theta_t M_t ~dB_t = 0.$$
Thus the It\^o product rule and the rule $dB_t{}^2 = dt$ gives
\begin{align*}
d(X_tM_t) &= X_t ~dM_t + M_t ~dX_t + dX_t~dM_t\\
&= -\theta_t X_t ~dB_t + M_t ~dB_t + \theta_t M_t ~dt - \theta_t M_t ~dB_t{}^2\\
&= (M_t - \theta_tX_t) ~dB_t.
\end{align*}
Therefore $XM$ is a local martingale. We must show that this property holds globally; in other words, for every $T > 0$ we have
\begin{equation}
\label{global martingale}
E\int_0^T (M_t - \theta_tX_t)^2 ~dt < \infty.
\end{equation}
To see this, we observe that
$$M_t^2 = \exp\left(-2\int_0^t \theta_s ~dB_s - \int_0^t \theta_s^2 ~ds\right)$$
and hence
$$d(M_t^2) = -2\theta_tM_t ~dB_t$$
by the It\^o product rule.
Since $\theta$ is uniformly bounded, let $C = \sup_t ||\theta_t||_{L^\infty} < \infty$. Then
\begin{align*}
EM_t^2 &= E \exp\left(-2\int_0^t \theta_s ~dB_s - \int_0^t \theta_s^2 ~ds\right)\\
&\leq E \exp\left(2\int_0^t C~dB_s - \int_0^t ~ds\right)\\
&\leq E\exp(2CB_t)
\end{align*}
which is the moment-generating function of $N(0, t)$ with parameter $2C$; this implies
$$EM_t^2 \leq \exp(2C^2t^2) \leq \exp(2C^2T^2).$$
Similarly we have, by the Cauchy-Schwarz inequality,
\begin{align*}
E(\theta_t^2X_t^2) &\leq 2E(\theta_t^2 B_t^2) + 2E\left(\int_0^t \theta_t \theta_s ~ds\right)^2\\
&= 2C^2 \Var B_t + 2C^4t^2\\
&\leq 2C^2T + 2C^4T^2
\end{align*}
and so by the Cauchy-Schwarz inequality again,
$$E(M_t - \theta_t X_t)^2 \leq 2\exp(2C^2T^2) + 4C^2T + 4C^4T^2 < \infty.$$
By Tonelli's theorem, (\ref{global martingale}) follows.




\end{document}
