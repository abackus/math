
% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------

\documentclass[10pt]{article}

\usepackage[margin=.7in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{enumitem}
\usepackage{tikz-cd}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{algorithm2e}
\usepackage{verse,stmaryrd}
\usepackage{fancyvrb}

% Number systems
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\PP}{\mathbb P}
\newcommand{\FF}{\mathbb F}
\newcommand{\DD}{\mathbb D}
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\ch}{\operatorname{ch}}
\newcommand{\Con}{\operatorname{Con}}
\newcommand{\coker}{\operatorname{coker}}
\newcommand{\CVect}{\CC\operatorname{-Vect}}
\newcommand{\Cantor}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\card}{\operatorname{card}}
\newcommand{\dbar}{\overline \partial}
\newcommand{\diam}{\operatorname{diam}}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\End}{\operatorname{End}}
\DeclareMathOperator*{\esssup}{ess\,sup}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\Ind}{\operatorname{Ind}}
\newcommand{\Inn}{\operatorname{Inn}}
\newcommand{\interior}{\operatorname{int}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\mesh}{\operatorname{mesh}}
\newcommand{\LL}{\mathcal L_0}
\newcommand{\Leb}{\mathcal{L}_{\text{loc}}^2}
\newcommand{\Lip}{\operatorname{Lip}}
\newcommand{\ppGL}{\operatorname{PGL}}
\newcommand{\ppic}{\vspace{35mm}}
\newcommand{\ppset}{\mathcal{P}}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator*{\Res}{Res}
\newcommand{\Riem}{\mathcal{R}}
\newcommand{\RVect}{\RR\operatorname{-Vect}}
\newcommand{\Sch}{\mathcal{S}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\spn}{\operatorname{span}}
\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\TT}{\mathcal T}
\DeclareMathOperator{\tr}{tr}

% Calculus of variations
\DeclareMathOperator{\pp}{\mathbf p}
\DeclareMathOperator{\zz}{\mathbf z}
\DeclareMathOperator{\uu}{\mathbf u}
\DeclareMathOperator{\vv}{\mathbf v}
\DeclareMathOperator{\ww}{\mathbf w}

% Categories
\newcommand{\Ab}{\mathbf{Ab}}
\newcommand{\Cat}{\mathbf{Cat}}
\newcommand{\Group}{\mathbf{Group}}
\newcommand{\Module}{\mathbf{Module}}
\newcommand{\Set}{\mathbf{Set}}
\DeclareMathOperator{\Fun}{Fun}
\DeclareMathOperator{\Iso}{Iso}

% Complex analysis
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Logic
\renewcommand{\iff}{\leftrightarrow}
\newcommand{\Henkin}{\operatorname{Henk}}
\newcommand{\PA}{\mathbf{PA}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\proves}{\vdash}

% Group
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Out}{Out}

% Other symbols
\newcommand{\heart}{\ensuremath\heartsuit}

\DeclareMathOperator{\atanh}{atanh}

% Theorems
\theoremstyle{definition}
\newtheorem*{corollary}{Corollary}
\newtheorem*{falselemma}{Grader's ``Lemma"}
\newtheorem{exer}{Exercise}
\newtheorem{lemma}{Lemma}[exer]
\newtheorem{theorem}[lemma]{Theorem}


\usepackage[backend=bibtex,style=alphabetic,maxcitenames=50,maxnames=50]{biblatex}
\renewbibmacro{in:}{}
\DeclareFieldFormat{pages}{#1}

\begin{document}
\noindent
\large\textbf{Probability II, HW 5} \hfill \textbf{Aidan Backus} \\
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------\

\begin{exer}
Let $X_i$ be an iid sequence which is uniformly distributed on $(0, 1)$, and let $0 < c < 1/2$.
We want to find a stopping time $\tau$ which minimizes
$$E\left(\min_{0 \leq i \leq \tau} X_i + c\tau\right).$$
To this end, let
\begin{equation}
\label{Optimization1}
v^*(x) = \inf_\tau E\left(\min_{0 \leq i \leq \tau} X_i + c\tau\bigg|X_0 = x\right).
\end{equation}

Write down a Bellman equation for $v^*$.
Take an educated guess towards the optional stopping strategy, and use it to solve the Bellman equation explicitly.
Give a verification argument to show that $v^*$ indeed is the solution to the Bellman equation and compute the optional stopping time $\tau^*$.
If $c \geq 1/2$, what is the value function and optional stopping time?
\end{exer}

In this problem we will always refer to $v$ and its terms as \emph{costs}, as we want to minimize them.

The Bellman equation is
\begin{equation}
\label{Bellman1}
v(x) = \min(x, c + Ev(X_1)).
\end{equation}
To see this takes some work, but this is essentially a consequence of the sunk cost fallacy.

\begin{lemma}
\label{lemma11}
Suppose that $\tau$ is a stopping time.
Then either $X_\tau = \min_{i \leq \tau} X_i$, or there is another stopping time $\tau^\heart$ such that
$$E\left(\min_{0 \leq i \leq \tau^\heart} X_i + c\tau^\heart\right) \leq E\left(\min_{0 \leq i \leq \tau} X_i + c\tau\right).$$
\end{lemma}
\begin{proof}
Condition on the $X_i$ (and hence on $\tau$). Then there is $j < \tau$ with $X_j > X_\tau$,
$$E\left(\min_{0 \leq i \leq \tau} X_i + c\tau\right) = X_j + c\tau < X_j + cj$$
and we could replace $\tau$ by a better stopping time $\tau^\heart$ by instead stopping at time $j$.
\end{proof}

Therefore, if we are in fact stopping at an optimal time, the cost of stopping is exactly $x$, and the cost of continuation does not depend on $x$ at all, and therefore must be $c + Ev(X_1)$, proving (\ref{Bellman1}).

Reasoning by the sunk cost fallacy, we might as well assume that it is currently time $0$.
In that case, if
$$\alpha = c + Ev(X_1)$$
and $x \leq \alpha$, then we should stop; otherwise we should continue. Thus if the Bellman equation (\ref{Bellman1}) has a solution $v$, the optimal stopping time $\tau^*$ is exactly the first hitting time of $x \leq \alpha$.

Now we compute $\alpha$. In fact,
\begin{align*}
\alpha &= c + Ev(X_1) = c + \int_0^1 v(x) ~dx = c + \int_0^1 x \wedge \alpha ~dx\\
&= c + \alpha - \frac{\alpha^2}{2}.
\end{align*}
Therefore $\alpha^2/2 = c$, or in other words $\alpha = \sqrt{2c}$. Therefore the solution to (\ref{Bellman1}) is
\begin{equation}
\label{Cost1}
v^*(x) = \min(x, \sqrt{2c})
\end{equation}
and $\tau^*$ is the first hitting time of $x \leq \sqrt{2c}$.

Now we run a verification argument:
\begin{lemma}
If $v^*$ satisfies (\ref{Cost1}), then $v^*$ is the cost function of the optimization problem (\ref{Optimization1}).
In particular, $\tau^*$ is the optimal stopping time associated to (\ref{Optimization1}).
\end{lemma}
\begin{proof}
We have already argued that if $\tau$ is the optimal stopping time associated to (\ref{Optimization1}), then
$$E\left(\min_{0 \leq i \leq \tau} X_i + c\tau\bigg| X_0 = x\right) = E(X_\tau + c\tau|X_0 = x).$$
We will call such stopping times admissible.
To show that $\tau^*$ is the optimal stopping time, it suffices to show that $\tau^*$ minimizes (\ref{Optimization1}) among admissible stopping times.
Indeed, $\tau^*$ is clearly admissible (since it only remembers the value of $X_n$ at time $n$, and ``forgets" $X_1, \dots, X_{n-1}$), so if $\tau^*$ is a minimizer among admissible
 stopping times, if there is an optimal stopping time, then $\tau^*$ is an optimal stopping time by Lemma \ref{lemma11}.
On the other hand, if there is not an optimal stopping time, then there is an infimizing sequence of stopping times $\tau_m$ which must eventually be more optimal than any admissible stopping time, but then by Lemma \ref{lemma11} we can replace $\tau_m$ by an admissible stopping time and improve its performance, which contradicts the fact that $\tau_m$ was already better than every admissible stopping time.

Let $\tau$ be an admissible stopping time and let $M_n = v^*(X_n) + c/2$. Then
$$E(M_{n+1}|\mathcal F_n) = E(v^*(X_{n+1})|\mathcal F_n) + \frac{c}{2} = E(v^*(X_{n + 1})) + \frac{c}{2} = \sqrt{2c} + \frac{c}{2}.$$
Thus
$$M_n = \min(X_n, \sqrt{2c}) + \frac{c}{2} \leq \sqrt{2c} + \frac{c}{2} = E(M_{n+1}|\mathcal F_n),$$
so that $M$ is a submartingale. Furthermore, $0 < M \leq \sqrt{2c} + c/2$, so $M$ is bounded, and by optional stopping,
$$E(v^*(X_\tau)) = E(M_\tau) - \frac{c}{2} \geq E(M_0) - \frac{c}{2} = v^*(X_0).$$
In particular,
\begin{align*}v^*(x)&\leq E(\min(X_\tau, \sqrt{2c})|X_0 = x) \leq E(X_\tau|X_0 = x) \\
&\leq E(X_\tau + c\tau|X_0 = x) = E\left(\min_{0 \leq i \leq \tau} X_i + c\tau\bigg| X_0 = x\right),
\end{align*}
since $\tau$ is admissible.

Thus we just need to show that
$$v^*(x) \geq E\left(\min_{0 \leq i \leq \tau^*} X_i + c\tau^*\bigg| X_0 = x\right).$$
In fact, the stopped submartingale $M^{\tau^*}$ is a martingale, since
$$E(M_{(n+1) \wedge \tau^*}|\mathcal F_n) = \sum_{k=0}^n M_k 1_{\tau^*=k} + 1_{\tau^* > n} E(M_{n+1}|\mathcal F_n)$$
and
$$1_{\tau^* > n} E(M_{n+1}|\mathcal F_n) = M_n$$
so that $E(M^{\tau^*}_{n+1}|\mathcal F_n) = M_n^{\tau^*}$.
Therefore by optional stopping
$$E(M_n^{\tau^*}|X_0 = x) = E(M_0|X_0 = x) = v^*(x) + \frac{c}{2}$$
and so by dominated convergence and admissibility
$$E\left(\min_{0 \leq i \leq \tau^*} X_i + c\tau^*\bigg| X_0 = x\right) = E(v^*(X_{\tau^*})|X_0 = x) = E(M_{\tau^*}|X_0 = x) - \frac{c}{2} = v^*(x)$$
which implies that $\tau^*$ is best possible.
\end{proof}

If $c \geq 1/2$, then the above argument goes through, but now $\tau^*$ is the first hitting time of $x \leq 1$, which is always $0$. Therefore the cost function is just $v(x) = x$.



\begin{exer}
Let $Y$ be an iid Bernoulli random sequence. Let $X$ be the simple random walk with initial data $x_0 \in \ZZ$ determined by $Y$.
Let $\beta \in (0, 1)$ be a discounting factor. We want to maximize $E(\beta^\tau(1 - e^{X_\tau})^+)$ over all stopping times $\tau$.

Write down the value function and Bellman equation. Make an educated guess on the optimal stopping strategy. Solve the Bellman equation explicitly. Prove that the solution is the value function and the strategy given is the optional stopping strategy.
\end{exer}

The Bellman equation is
$$v(x) = \max\left((1 - e^x)^+, \frac{\beta}{2}(v(x + 1) + v(x - 1))\right).$$
To see this, we argue as in the first exercise to see that the value of stopping at time $0$ is $(1 - e^x)^+$ and the value of continuation at time $0$ is $\beta Ev(x + Y_1)$, and as $Y_1$ is Bernoulli the claim follows.
On the other hand, the value function is
$$v^*(x) = \sup_\tau E(\beta^\tau(1 - e^{X_\tau})^+).$$

So we expect the existence of $b(x)$ such that $(1 - e^x)^+$ is greater than the value of continuation exactly if $x \leq b(x)$.
Note that unlike in the previous problem, $b(x)$ is allowed to depend on $x$, since the value at time $n + 1$ depends on the value at time $n$, unlike in the previous problem where they were independent.
Furthermore $b(x)$ will be either an integer or $\pm \infty$, since $x$ itself only takes integer values.
Then the optimal stopping time $\tau^*$ will be the first hitting time of $x \leq b(x)$.

Let $V(x) = \beta Ev^*(x + Y_1)$ be the value of continuation.
We will first show that $V \in [0, 1]$.
If it is not, then
$$(1 - e^x)^+ \leq 1 \leq V(x),$$
so the value of stopping is always at most than the value of continuation, and hence the optimal strategy is to never stop, $\tau^* = \infty$.
But $(1 - e^{X_n})^+ \in [0, 1]$, so in particular
$$\lim_{n \to \infty} \beta^n(1 - e^{X_n}) = 0.$$
Since this is the worst possible outcome, it cannot be the case that $\tau^* = \infty$, a contradiction.
If $V(x) < 0$, then either there exists a future time $n$ such that we will stop at time $n$, and get $\beta^n(1 - e^{X_n})^+ < 0$, a contradiction, or we will never stop, again leading to the contradiction $\tau^* = \infty$. Thus $V(x) \in [0, 1]$.
If $0 \leq V(x) \leq 1$, then we define the cutoff to be
$$b(x) = \lfloor \log (1 - V(x)) \rfloor,$$
which indeed is either an integer or $\pm\infty$.

Now we compute the value function $v^*$. If $x \leq b(x)$ then $v^*(x) = (1 - e^x)^+$.
On the other hand, if $x \geq b(x)$, then
$$- \frac{\beta}{2} v^*(x + 1) + v^*(x) - \frac{\beta}{2} v^*(x - 1) = 0$$
or, in other words, if $y + 1 \geq b(y + 1)$,
\begin{equation}
\label{recursive equation}
- \frac{\beta}{2} v^*(y + 2) + v^*(y + 1) - \frac{\beta}{2} v^*(y) = 0.
\end{equation}
The characteristic equation of the recursive equation (\ref{recursive equation})
$$- \frac{\beta}{2} \xi^2 + \xi - \frac{\beta}{2} = 0$$
has two roots
$$\xi_\pm = \frac{1 \pm \sqrt{1 - \beta^2}}{\beta}.$$
As $\beta \in (0, 1)$ is a discounting factor, it follows that
$$0 < \xi_- < 1 < \xi_+.$$
(Thus $\pm \log \xi_\pm > 0$.)
Therefore a basis of solutions to (\ref{recursive equation}) is $\xi_+^y, \xi_-^y$.
Plugging in $y = x - 1$ we conclude that there are $\alpha_\pm \in \RR$ such that
\begin{equation}
\label{AlphaBellman}
v^*(x) = \begin{cases}
\alpha_+ \xi_+^{x - 1} + \alpha_- \xi_-^{x - 1} , &x \geq b(x)\\
(1 - e^x)^+, &x \leq b(x).
\end{cases}
\end{equation}

Since the value of stopping $(1 - e^x)^+$ is nonincreasing in $x$, so is the value of continuation $V(x)$.
In particular, since
$$V(x) = \alpha_+ \xi_+^{x-1} + \alpha_- \xi_-^{x - 1}$$
and $\pm \log \xi_\pm > 0$, it must be the case that $\pm \alpha_\pm \leq 0$.
On the other hand, if $\alpha_+ < 0$, then
$$\lim_{x \to +\infty} v^*(x) = -\infty,$$
contradicting the fact that both the values of stopping and continuation are nonnegative.
Therefore $\alpha_+ = 0$ and we rewrite (\ref{AlphaBellman}) as
$$v^*(x) = \begin{cases}
\alpha \xi^x, &x \geq b(x)\\
(1 - e^x)^+, &x \leq b(x)
\end{cases}$$
where $\alpha = \alpha_-/\xi_- > 0$ is a constant to be determined and $\xi = \xi_- \in (0, 1)$.
In particular, if $x = b(x)$, we conclude the boundary condition
$$\alpha \xi^x = (1 - e^x)^+.$$

Having simplified $V$ to $V(x) = a\xi^x$, we can view this function as defined for $x \in \RR$.
We then extend $1 - \log V$ to a continuous function on $\RR$ by setting $1 - \log V(x) = 0$ if $x \geq 0$.
Since $b = \lfloor 1 - \log V \rfloor$ and $x < b(x)$ if $x$ is close enough to $-\infty$ (since there must be \emph{some} $x$ such that the value of stopping exceeds the value of continuation).

I'm not too sure what to do from here! It feels like I've simplified the value function \emph{too} much, but it seems like every line in my argument is valid.


\end{document}
