\documentclass[reqno,11pt]{amsart}
\usepackage[letterpaper, margin=1in]{geometry}
\RequirePackage{amsmath,amssymb,amsthm,graphicx,mathrsfs,url,slashed,subcaption}
\RequirePackage[usenames,dvipsnames]{xcolor}
\RequirePackage[colorlinks=true,linkcolor=Red,citecolor=Green]{hyperref}
\RequirePackage{amsxtra}
\usepackage{cancel}
\usepackage{tikz-cd}
%\usepackage[T1]{fontenc}

% \setlength{\textheight}{9.3in} \setlength{\oddsidemargin}{-0.25in}
% \setlength{\evensidemargin}{-0.25in} \setlength{\textwidth}{7in}
% \setlength{\topmargin}{-0.25in} \setlength{\headheight}{0.18in}
% \setlength{\marginparwidth}{1.0in}
% \setlength{\abovedisplayskip}{0.2in}
% \setlength{\belowdisplayskip}{0.2in}
% \setlength{\parskip}{0.05in}
%\renewcommand{\baselinestretch}{1.05}

\title{Vector-valued maps of least gradient}
\author{Aidan Backus}
\address{Department of Mathematics, Brown University}
\email{aidan\_backus@brown.edu}
\date{\today}

\newcommand{\NN}{\mathbf{N}}
\newcommand{\ZZ}{\mathbf{Z}}
\newcommand{\QQ}{\mathbf{Q}}
\newcommand{\RR}{\mathbf{R}}
\newcommand{\CC}{\mathbf{C}}
\newcommand{\DD}{\mathbf{D}}
\newcommand{\PP}{\mathbf P}
\newcommand{\MM}{\mathbf M}
\newcommand{\II}{\mathbf I}
\newcommand{\Torus}{\mathbf T}
\newcommand{\Hyp}{\mathbf H}
\newcommand{\Sph}{\mathbf S}
\newcommand{\Group}{\mathbf G}
\newcommand{\GL}{\mathbf{GL}}
\newcommand{\Orth}{\mathbf{O}}
\newcommand{\SpOrth}{\mathbf{SO}}
\newcommand{\Ball}{\mathbf{B}}

\newcommand*\dif{\mathop{}\!\mathrm{d}}
\newcommand*\Dif{\mathop{}\!\mathrm{D}}

\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\coker}{coker}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Vect}{Vect}
\DeclareMathOperator{\tr}{tr}


\DeclareMathOperator{\svd}{svd}
\DeclareMathOperator{\SVD}{SVD}

\newcommand{\Leaves}{\mathscr L}
\newcommand{\Lagrange}{\mathscr L}
\newcommand{\Hypspace}{\mathscr H}

\newcommand{\Chain}{\underline C}

\newcommand{\Two}{\mathrm{I\!I}}
\newcommand{\Ric}{\mathrm{Ric}}

\newcommand{\normal}{\mathbf n}
\newcommand{\radial}{\mathbf r}
\newcommand{\evect}{\mathbf e}
\newcommand{\vol}{\mathrm{vol}}

\newcommand{\diam}{\mathrm{diam}}
\newcommand{\Ell}{\mathrm{Ell}}
\newcommand{\inj}{\mathrm{inj}}
\newcommand{\Lip}{\mathrm{Lip}}
\newcommand{\MCL}{\mathrm{MCL}}
\newcommand{\Riem}{\mathrm{Riem}}

\newcommand{\frkg}{\mathfrak g}

\newcommand{\Mass}{\mathbf M}
\newcommand{\Comass}{\mathbf L}

\newcommand{\Min}{\mathrm{Min}}
\newcommand{\Max}{\mathrm{Max}}

\newcommand{\dfn}[1]{\emph{#1}\index{#1}}

\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

\newcommand{\loc}{\mathrm{loc}}
\newcommand{\cpt}{\mathrm{cpt}}

\def\Japan#1{\left \langle #1 \right \rangle}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{badtheorem}[theorem]{``Theorem"}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{sublemma}[theorem]{Sublemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{assumption}[theorem]{Assumption}

\newtheorem{mainthm}{Theorem}
\renewcommand{\themainthm}{\Alph{mainthm}}

\newtheorem{claim}{Claim}[theorem]
\renewcommand{\theclaim}{\thetheorem\Alph{claim}}
% \newtheorem*{claim}{Claim}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{notation}[theorem]{Notation}

\newtheorem{exercise}[theorem]{Discussion topic}
\newtheorem{homework}[theorem]{Homework}
\newtheorem{problem}[theorem]{Problem}

\makeatletter
\newcommand{\proofpart}[2]{%
  \par
  \addvspace{\medskipamount}%
  \noindent\emph{Part #1: #2.}
}
\makeatother



\numberwithin{equation}{section}


% Mean
\def\Xint#1{\mathchoice
{\XXint\displaystyle\textstyle{#1}}%
{\XXint\textstyle\scriptstyle{#1}}%
{\XXint\scriptstyle\scriptscriptstyle{#1}}%
{\XXint\scriptscriptstyle\scriptscriptstyle{#1}}%
\!\int}
\def\XXint#1#2#3{{\setbox0=\hbox{$#1{#2#3}{\int}$ }
\vcenter{\hbox{$#2#3$ }}\kern-.6\wd0}}
\def\ddashint{\Xint=}
\def\dashint{\Xint-}

\usepackage[backend=bibtex,style=alphabetic,giveninits=true]{biblatex}
\renewcommand*{\bibfont}{\normalfont\footnotesize}
\addbibresource{least_gradient_maps.bib}
\renewbibmacro{in:}{}
\DeclareFieldFormat{pages}{#1}

\newcommand\todo[1]{\textcolor{red}{TODO: #1}}


\begin{document}
\begin{abstract}

\end{abstract}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Rank-one maps}
\begin{theorem}
Let $u: M \to \Torus^d$ be a continuous $sv^1$ least gradient map of rank $1$.
Then there exists a geodesic circle $\Gamma: \RR \to \Torus^d$ and a chain $\lambda$ of minimal hypersurfaces in $M$ such that $\lambda$ is the set of level sets of $u$, and $\Gamma$ is the image of $u$. 
\end{theorem}
\begin{proof}
Let's assume that $u$ is smooth for now and think about approximations later. (Maybe the approximations are so good we don't even need to assume that $u$ is continuous.)
Locally (say in an open set $U$) $u$ looks like $\gamma \circ v$ where $v$ is a scalar field and $\gamma$ is a curve.
WLOG $\gamma$ is constant speed with domain $[0, 1]$, so if it has length $\ell$ then by the chain rule and the fact that $\dif u = \dif v \otimes \dif \gamma$,
$$\int_U |\dif u|_{sv^1} \dif V = \int_U \sigma_1(\dif u) \dif V = \int_U |\dif v| |\dif \gamma| \dif V = \ell \int_U |\dif v| \dif V.$$
If $v$ does not have least gradient, then we can decrease this quantity without affecting $u|_{\partial U}$ or $\ell$ by replacing $v$ with its least gradient version.
Note that if neither of those quantities are changed, and $U$ is a small ball, then we're only locally deforming $u$, hence we are not changing its homotopy class.
So $v$ has least gradient and hence $H = 0$ and $\lambda$ is a MS chain.
Then the only free parameter is $\ell$.
But then the Euler-Lagrange equations kick in and force the geodesic curvature of $\gamma$ to be $0$ (since $H = 0$), which determines $\ell$.
\end{proof}

We never actually used the fact that the homotopy class is an integer matrix so this works for sections of flat affine bundles too, including the bundle of Noetherian conservation laws.

\begin{corollary}
(I think?) Lots of Dirichlet data doesn't admit continuous $sv^1$ least gradient maps of rank $1$.
For example, the identity map on an exact hypersurface.
\end{corollary}
\begin{proof}
The image of any extension of such a map contains a curve which is not a geodesic.
\end{proof}

\section{The Euler-Lagrange equations}
We can go back and do this for all differentiable norms too.

\subsection{Measurable selection of the SVD}
This part is hella cursed but I'd rather get it over with now.
Let $\mathscr K(X)$ be the Hausdorff hyperspace of closed subsets of a metric space $X$.
Let $S(d, c) := O(c) \times D(d, c) \times O(d)$ be the space of possible SVDs of $d \times c$ matrices.
We have a map 
$$\SVD: \RR^{d \times c} \to \mathscr K(S(d, c))$$
which sends a matrix to its set of SVD's.
This set really is closed, basically because matrix multiplication is continuous (so a sequence of SVDs converges to an SVD).

\begin{lemma}
Let $f: M \to \RR^{d \times c}$ be $\mu$-measurable.
Then there is a measurable map $\svd \circ f: M \to S(d, c)$ such that for $\mu$-almost every $x$, $\svd \circ f(x)$ is an SVD of $f(x)$.
\end{lemma}
\begin{proof}
We want to find a measurable selection of $\SVD \circ f$.
$(U, D, V) \mapsto UDV^\dagger$ is a smooth submersion (I think?), so for $\varepsilon > 0$ small we can find an pseudoinverse of the restriction of this map to $B((U, D, V), \varepsilon)$, which is then a continuous selection $\svd_\alpha$ of $\SVD$ defined on an open set $E_\alpha$.
Since any matrix has an SVD, we can cover $\RR^{d \times c}$ by such neighborhoods, and we may assume that this cover is locally finite, and in fact is indexed by $\NN$.
By setting $\svd(A)$ to be $\svd_\alpha(A)$ where $\alpha$ is minimal among those $\beta$ with $A \in E_\beta$, we get a measurable selection $\svd$ of $\SVD$.

To see that $g := \svd \circ f$ is measurable, take a small open set $W \subseteq S(d, c)$.
Then $Z_\alpha := \svd_\alpha^{-1}(W)$ is open for any $\alpha$ and
$$g^{-1}(W) = \bigcup_\alpha f^{-1}(E_\alpha) \cap \left[\bigcap_{\beta > \alpha} M \setminus f^{-1}(E_\beta)\right] \cap f^{-1}(Z_\alpha)$$
which is clearly measurable.
\end{proof}

We write $R(A) := U(A) V(A)^\dagger$ which is not well-defined but by the above has a measurable selection if $A$ is measurable.

\begin{definition}
$u$ is a \dfn{calibrated $sv^1$ $1$-harmonic map}, or in other words
$$\nabla^\dagger R(\nabla u) = 0$$
in the \dfn{calibrated sense}, if there exists a Lebesgue and $|\nabla u|$-measurable matrix field $R$ with 
$$\||R|_{sv^\infty}\|_{L^\infty} \leq 1,$$ $\nabla^\dagger R = 0$, and $R$ is a measurable selection of $R(\nabla u)$.
\end{definition}

\begin{theorem}
$u$ is a $sv^1$ map of least gradient iff $u$ is a calibrated $sv^1$ $1$-harmonic map.
\end{theorem}

\subsection{Approximation by svp,q-harmonic maps}


\subsection{Applications of the EL equations}

\begin{corollary}
Tensor products of $sv^1$ least gradient maps have $sv^1$ least gradient.
\end{corollary}

\begin{corollary}
The identity map is $sv^p$ least gradient.
\end{corollary}

\section{Monotonicity formula}
Up to SVD technicalities, it's the same as for least gradient functions but only for $sv^p$, $p \leq 2$.
No idea if it has applications though.

\printbibliography

\end{document}
