\documentclass[reqno,10pt]{amsart}
\usepackage[letterpaper, margin=1in]{geometry}
\RequirePackage{amsmath,amssymb,amsthm,graphicx,mathrsfs,url,slashed}
\RequirePackage[usenames,dvipsnames]{xcolor}
\RequirePackage[colorlinks=true,linkcolor=Red,citecolor=Green]{hyperref}
\RequirePackage{amsxtra}
\usepackage{cancel}
\usepackage{tikz-cd, mathabx}

% \setlength{\textheight}{9.3in} \setlength{\oddsidemargin}{-0.25in}
% \setlength{\evensidemargin}{-0.25in} \setlength{\textwidth}{7in}
% \setlength{\topmargin}{-0.25in} \setlength{\headheight}{0.18in}
% \setlength{\marginparwidth}{1.0in}
% \setlength{\abovedisplayskip}{0.2in}
% \setlength{\belowdisplayskip}{0.2in}
% \setlength{\parskip}{0.05in}
%\renewcommand{\baselinestretch}{1.05}

\title{The higher-dimensional fractal uncertainty principle via randomized Dolgapyat iteration}
\author{Bacon, Lettuce, and Tomato}
\date{August 2022}

\newcommand{\NN}{\mathbf{N}}
\newcommand{\ZZ}{\mathbf{Z}}
\newcommand{\QQ}{\mathbf{Q}}
\newcommand{\RR}{\mathbf{R}}
\newcommand{\CC}{\mathbf{C}}
\newcommand{\DD}{\mathbf{D}}
\newcommand{\PP}{\mathbf P}
\newcommand{\MM}{\mathbf M}
\newcommand{\II}{\mathbf I}
\newcommand{\Hyp}{\mathbf H}
\newcommand{\Sph}{\mathbf S}
\newcommand{\Group}{\mathbf G}
\newcommand{\GL}{\mathbf{GL}}
\newcommand{\Orth}{\mathbf{O}}
\newcommand{\SpOrth}{\mathbf{SO}}
\newcommand{\Ball}{\mathbf{B}}

\DeclareMathOperator*{\Expect}{\mathbf E}

\DeclareMathOperator{\avg}{avg}
\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\cent}{center}
\DeclareMathOperator{\ch}{ch}
\DeclareMathOperator{\codim}{codim}
\DeclareMathOperator{\Cyl}{Cyl}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\Exc}{Exc}
\newcommand{\ext}{\mathrm{ext}}
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Iso}{Iso}
\DeclareMathOperator{\Jac}{Jac}
\DeclareMathOperator{\Lip}{Lip}
\DeclareMathOperator{\Met}{Met}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\rad}{rad}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Rm}{Rm}
\DeclareMathOperator{\Hess}{Hess}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Radon}{Radon}
\DeclareMathOperator*{\Res}{Res}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\singsupp}{sing~supp}
\DeclareMathOperator{\Spec}{Spec}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Tan}{Tan}
\newcommand{\tr}{\operatorname{tr}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\Mink}{\mathbf m}
\newcommand{\Ric}{\mathrm{Ric}}
\newcommand{\Riem}{\mathrm{Riem}}
\newcommand*\dif{\mathop{}\!\mathrm{d}}
\newcommand*\Dif{\mathop{}\!\mathrm{D}}
\newcommand{\LapQL}{\Delta^{\mathrm{ql}}}

\newcommand{\dbar}{\overline \partial}

\DeclareMathOperator{\hull}{hull}

\DeclareMathOperator{\Div}{div}
\DeclareMathOperator{\Gram}{Gram}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\Ell}{Ell}
\DeclareMathOperator{\WF}{WF}

\newcommand{\Lagrange}{\mathscr L}
\newcommand{\DirQL}{\mathscr D^{\mathrm{ql}}}
\newcommand{\DirL}{\mathscr D}

\newcommand{\Hilb}{\mathcal H}
\newcommand{\Homology}{\mathrm H}
\newcommand{\normal}{\mathbf n}
\newcommand{\radial}{\mathbf r}
\newcommand{\evect}{\mathbf e}
\newcommand{\vol}{\mathrm{vol}}

\newcommand{\pic}{\vspace{30mm}}
\newcommand{\dfn}[1]{\emph{#1}\index{#1}}

\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

\newcommand{\loc}{\mathrm{loc}}
\newcommand{\cpt}{\mathrm{cpt}}

\def\Japan#1{\left \langle #1 \right \rangle}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{badtheorem}[theorem]{``Theorem"}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{sublemma}[theorem]{Sublemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{assumption}[theorem]{Assumption}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{notation}[theorem]{Notation}

\newtheorem{exercise}[theorem]{Discussion topic}
\newtheorem{homework}[theorem]{Homework}
\newtheorem{problem}[theorem]{Problem}

\makeatletter
\newcommand{\proofpart}[2]{%
  \par
  \addvspace{\medskipamount}%
  \noindent\emph{Part #1: #2.}
}
\makeatother

\newtheorem{ack}{Acknowledgements}

\numberwithin{equation}{section}


% Mean
\def\Xint#1{\mathchoice
{\XXint\displaystyle\textstyle{#1}}%
{\XXint\textstyle\scriptstyle{#1}}%
{\XXint\scriptstyle\scriptscriptstyle{#1}}%
{\XXint\scriptscriptstyle\scriptscriptstyle{#1}}%
\!\int}
\def\XXint#1#2#3{{\setbox0=\hbox{$#1{#2#3}{\int}$ }
\vcenter{\hbox{$#2#3$ }}\kern-.6\wd0}}
\def\ddashint{\Xint=}
\def\dashint{\Xint-}

\usepackage[backend=bibtex,style=numeric]{biblatex}
\renewcommand*{\bibfont}{\normalfont\footnotesize}
\addbibresource{fup.bib}
\renewbibmacro{in:}{}
\DeclareFieldFormat{pages}{#1}


\begin{document}
\begin{abstract}
    Dolgopyat's method...
\end{abstract}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \tableofcontents

\section{Introduction}
An open cocompact $d+1$-dimensional hyperbolic manifold $M$ is said to have \dfn{essential spectral gap} $\beta \geq 0$ if the Laplace-Beltrami operator $-\Delta_M$ has only finitely many resonances $\lambda$ with imaginary part $\Im \lambda > -\beta$.
Aside from being a problem of intrinsic interest, lower bounds on the essential spectral gap have applications to number theory \cite{Bourgain_2011} and general relativity \cite[Chapter 5]{dyatlov2019mathematical}.
In a seminal 2016 paper, Dyatlov and Jin reduced the problem of essential spectral gaps to a \dfn{fractal uncertainty principle} for the limit fractal $\Lambda \subset \RR^d$ of the fundamental group $\pi_1(M)$: it suffices to show that
$$||1_{\Lambda(h)} \mathcal B_h 1_{\Lambda(h)}||_{L^2(\RR^d) \righttoleftarrow} \lesssim h^\beta,$$
where $h > 0$ is a semiclassical parameter, $\Lambda(h)$ is the $h$-neighborhood of $\Lambda$, and $\mathcal B_h$ is the composition of a certain semiclassical Fourier integral operator with the semiclassical Fourier transform \cite{Dyatlov_2016}.
If $\Lambda$ has dimension $\delta \in [0, d]$, then has the trivial bound $\beta \geq \max(0, d/2 - \delta)$, but the fractal uncertainty principle asserts that, if $\delta \in (0, d)$, then in fact $\beta > \max(0, d/2 - \delta)$.

Since the publication of \cite{Dyatlov_2016}, the fractal uncertainty principle has been proven unconditionally for the surface case, $d = 1$ \cite{Bourgain_2018, Dyatlov_2018}.
However, the fractal uncertainty principle is false in general for $d \geq 2$: a counterexample is $\mathcal B_h$ the semiclassical Fourier transform and $\Lambda$ the union of two Cantor sets lying in orthogonal lines in $\RR^2$.
Some special cases which are known, however, are when $\delta > d/2$ and $\Lambda$ decomposes into a product of Ahlfors-David fractals in $\RR$, or when $d \geq 3$ is odd and $\delta \approx d/2$ (thus $\delta$ is far from an integer) \cite{Han_2020, Cladek_2021}.

Here we address the case $d \geq 2$, $\delta \leq d/2$, in case $\Lambda$ is not ``self-orthogonal'' in a certain sense, allowing us to avoid the standard counterexample given above.
This in particular includes the notorious open problem of $d = 2$, $\delta = 1$.

Let us sketch the proof of the analogous case of $d = 1$, $\delta \leq 1/2$, established by Dyatlov and Jin \cite{Dyatlov_2018}.
The proof uses \dfn{Dolgapyat's method}, a technique due to Dolgapyat and Naud \cite{10.2307/121012, ASENS_2005_4_38_1_116_0} for establishing essential spectral gaps via induction on scale.
It suffices to show that, for $\mu$ the Ahlfors-David measure of $\Lambda$,
$$||\mathcal B_h||_{L^2(\mu) \righttoleftarrow} \lesssim h^{\varepsilon_0}$$
for some $\varepsilon_0 > 0$.
One begins by realizing the fractal $\Lambda$ as the space of paths through a certain tree $V$.
Here the vertices of $V$ are unions $I \subset \RR^d$ of $L$-adic tiles which intersect $\Lambda$ and tile $\RR^d$, and are weighted by their measure $\mu(I)$.
One lets $\{I_a: a \in A\}$ be the set of children of $I$, and establishes a uniform lower bound $\mu(I_a) \gtrsim L^{-\delta} \mu(I)$.
Thus all children $I_a$ are comparable in measure, so it suffices to get a gain in a certain norm $C_\theta(I)$ in a single child to apply induction on scale.
However, one can find two children $I_a, I_{a'}$ which are ``evenly spaced'' from each other, in the sense that there is some cancellation between the phase $\Phi$ of the semiclassical Fourier integral operator at $I_a, I_{a'}$, ensuring a gain.

There are two serious difficulties with this approach for $d \geq 2$:
\begin{enumerate}
\item The implied constant in the bound $\mu(I_a) \gtrsim L^{-\delta} \mu(I)$ includes a power of $(1 - \delta)^{-1}$ which degenerates for $\delta \geq 1$. Geometrically this corresponds to the fact that fractals of dimension $\delta \geq 1$ may not be totally disconnected and so there can be ``long and narrow'' vertices $I$.
\item In order to select $a, a'$ one needs a ``reverse Cauchy-Schwarz inequality'' $|x|, |y| \gtrsim |\Phi(x, y)|$ which is clearly false for $d = 2$, $\Phi(x, y) = x \cdot y$. This is the reason why Dolgapyat's method fails for self-orthogonal fractals.
\end{enumerate}
We overcome these challenges using a technique that we call \dfn{randomized Dolgapyat iteration}.
We begin by requiring that vertices of $V$ are $L$-adic tiles, rather than unions thereof.
In that case it is possible for $p_a := \mu(I_a)/\mu(I)$ to be arbitrarily small.
However, by setting $\PP(\{a\}) = p_a$ we can endow $A$ with the structure of a probability space, and then, by a pigeonholing argument, with overwhelming probability in the limit $L \to \infty$, for a randomly selected $a$, we have $p_a \gtrsim L^{-\delta}$.
Now given $a, b$, where $b \in B$, $\{J_b: b \in B\}$ the children of a vertex $J$, we have for fractals which are not self-orthogonal that, for some small but uniformly positive probability, randomly selected $a', b'$, and for $x \in I_a - I_{a'}$, $y \in J_b - J_{b'}$ that the reverse Cauchy-Schwarz inequality holds.
In conclusion, for randomly selected $a, a', b, b'$, we have for some small but nonzero probability the desired lower bounds on $p_\bullet$ and the reverse Cauchy-Schwarz inequality.
From here the argument is essentially identical to \cite{Dyatlov_2018}.

\section{Randomized Dolgapyat iteration}
Let
$$F_J(x) = \mu_Y(J)^{-1} \int_J \exp\left(i\frac{\Phi(x, y) - \Phi(x, y_J)}{h}\right) G(x, y) f(y) \dif \mu_Y(y).$$
Here $y_J$ is the barycenter of $J$.
Let $\{I_a: a \in A\}$ and $\{J_b: b \in B\}$ be sets of children and $p_a := \mu_X(I_a)/\mu_X(I)$, $q_b := \mu_Y(J_b)/\mu_Y(J)$.
We impose $A, B$ with the probability measures with weights $p_a, q_b$.

When talking about $C_\theta(I)$ we write $|I|$ for the diameter of a tile $I$.

\begin{lemma}\label{theta assumptions}
There exists $0 < \theta \ll 1$ which only depends on $X, Y$ such that at every scale,
\begin{equation}\label{assumption on theta}
\max\left(||e^{i\Psi_b} F_{J_b}||_{C_\theta(I_a)}, 2\theta |I_a| \cdot ||(e^{i\Psi_b} F_{J_b})'||_{C^0(I_a)}\right) \leq ||F_{J_b}||_{C_\theta(I)},
\end{equation}
\begin{equation}\label{mean value theorem}
\frac{|f(x) - f(x')|}{|x - x'|} \leq \frac{||f||_{C_\theta(I)}}{\theta |I|},
\end{equation}
and if $H(J) \leq 0$ then $||F_J||_{C_\theta(I)} \leq 1$.
\end{lemma}

\begin{theorem}
Assume that for every $K \leq -\log_L h$, $I \in V(X)$, $J \in V(Y)$ such that $H(I) + H(J) + 1 = K$, and there exists $C > c > 0$ such that for every $L \gg 1$ there exists $0 < \rho = \rho(L) \ll 1$ such that for every $K \leq -\log_L h$, and every $I \in V(X)$, $J \in V(Y)$, such that $K = H(I) + H(J) + 1$, let $x_a := \argmax_{I_a} |F_J|$ and $y_b := y_{J_b}$. Then, drawing $a, a' \in A$ and $b, b' \in B$ at random, the event that
\begin{equation}\label{comparable lengths}
C^{-1} L^{-2/3} \leq L^{H(I)} |x_a - x_{a'}|, L^{H(J)} |y_b - y_{b'}| \leq C L^{-2/3}
\end{equation}
holds, and that we have the reverse Cauchy-Schwarz inequality
\begin{equation}\label{Reverse Cauchy Schwarz}
|\Phi(x_a, y_b) - \Phi(x_{a'}, y_b) - \Phi(x_a, y_{b'}) + \Phi(x_{a'}, y_{b'})| \geq c |x_a - x_{a'}| \cdot |y_b - y_{b'}|,
\end{equation}
has probability $\geq \rho$.

Then there exists $0 < \varepsilon_1 \leq \rho^2$ independent of $K$ such that
\begin{equation}\label{improved volume bound}
||F_J||_{C_\theta(I)}^2 \leq (1 - \varepsilon_1) \Expect ||F_{J_b}||_{C_\theta(I)}^2.
\end{equation}
\end{theorem}
\begin{proof}
We first compute
$$F_J = \Expect e^{i\Psi_b} F_{J_b}.$$
So by (\ref{assumption on theta}) and by reasoning identically to \cite[Lemma 3.3]{Dyatlov_2018},
\begin{equation}\label{Expectation bound}
||F_J||_{C_\theta(I_a)}^2 \leq \left(\Expect ||F_{J_b}||_{C_\theta(I)}\right)^2 \leq \Expect ||F_{J_b}||_{C_\theta(I)}^2 =: R.
\end{equation}
Taking expectations in (\ref{Expectation bound}), we obtain the volume bound $||F_J||_{C_\theta(I)} \leq \sqrt R$, and
\begin{equation}\label{Variance bound}
\sigma^2 := \Expect ||F_{J_b}||_{C_\theta(I)}^2 - \Expect ||F_J||_{C_\theta(I_a)}^2 \geq \Var ||F_{J_b}||_{C_\theta(I)}.
\end{equation}
By \cite[(3.16)]{Dyatlov_2018},
$$||F_J||_{C_\theta(I)}^2 = \Expect ||F_J||_{C_\theta(I_a)}^2.$$
Combining this with (\ref{Variance bound}), we see that in order to prove (\ref{improved volume bound}), it suffices to show that there exists $\varepsilon_1 > 0$ such that $\sigma^2 \geq \varepsilon_1 R$.

Let $F_{ab} := F_{J_b}(x_a)$, $\omega_{ab} := \Psi_b(x_a)$, thus 
$$F_J(x_a) = \Expect e^{i\omega_{ab}} F_{ab}.$$
By (\ref{Expectation bound}) and definition of $R$ we have 
\begin{equation}\label{Expectation bound 2}
\Expect ||F_{J_b}||_{C_\theta(I)} \leq \sqrt R.
\end{equation}
By Chebyshev's inequality and (\ref{Expectation bound 2}, \ref{Variance bound}),
$$\Pr(||F_{J_b}||_{C_\theta(I)} \leq 2\sqrt R) \geq 1 - \frac{R}{\sigma^2}$$
and hence, by Lemma \ref{theta assumptions}, we have with probability $\geq 1 - R/\sigma^2$ that
\begin{equation}\label{discrepancy upper bound}
|F_{ab} - F_{a'b}| \leq \frac{2\sqrt R}{\theta} L^{H(I)} |x_a - x_{a'}|.
\end{equation}

\begin{claim}\label{variance and iids claim}
Let $f_{ab} := e^{i\omega_{ab}} F_{ab}$. If $\sigma \leq \sqrt{R/2}$ then
$$\Expect |F_{ab}|^2 \geq R - 2\sigma^2 + \frac{1}{2} \Expect |f_{ab} - f_{ab'}|^2.$$
\end{claim}
\begin{proof}[Proof of claim]
We first use Lemma \ref{theta assumptions} as in \cite[Lemma 3.5]{Dyatlov_2018} to get
$$||F_J||_{C_\theta(I_a)}^2 \leq \frac{1}{2} \left(R + ||F_J||_{C^0(I_a)}^2\right).$$
I don't actually understand Dyatlov--Jin here but let's just assume that this works for now.
The argument seems to only use Cauchy-Schwarz and properties of $C_\theta$, not anything like a contradiction assumption or uniform lower bound on the measures of each child.
Anyways, after taking expectations and applying the definition of $F_{ab}$, we get 
$$\Expect ||F_J||_{C^0(I_a)}^2 \geq 2 \Expect ||F_J||_{C_\theta(I_a)}^2 - R = R - 2\sigma^2.$$
Therefore, since $|F_J(x_a)| = ||F_J||_{C^0(I_a)}$,
$$|\Expect f_{ab}|^2 = \Expect |F_J(x_a)|^2 \geq R - 2\sigma^2.$$
Therefore 
$$\Expect |F_{ab}|^2 = \Expect |f_{ab}|^2 = |\Expect f_{ab}|^2 + \Var f_{ab} \geq R - 2\sigma^2 + \Var f_{ab}.$$
On the other hand, $f_{ab}, f_{ab'}$ are iid samples, so $2\Var f_{ab} = \Expect |f_{ab} - f_{ab'}|^2$.
\end{proof}

\begin{claim}\label{Fab lower bound}
If $\sigma, \varepsilon \ll 1$, then the probability that $|F_{ab}| \geq \sqrt R/2$ and $|f_{ab} - f_{ab'}| \lesssim \sqrt R/\varepsilon$ is $\geq 1 - O(\sigma^2)$.
\end{claim}
\begin{proof}
This should follow from James' argument and Claim \ref{variance and iids claim}. We probably need to get precise dependence on these constants though.
\end{proof}

Now let
$$\tau := \omega_{ab} - \omega_{a'b} - \omega_{ab'} + \omega_{a'b'}.$$
With probability $\geq \rho$, we have the bounds (\ref{comparable lengths}) and
$$h |\tau| \geq c|x_a - x_{a'}| \cdot |y_b - y_{b'}|.$$
In particular, 
$$|\tau| \sim L^{-1/3} \ll \pi$$
(need to work out the dependencies here) with probability $\geq \rho$.
By Claim \ref{Fab lower bound} we get
$$|F_{ab}| \cdot |e^{i\tau} - 1| \geq \frac{|\tau| \sqrt R}{\pi} \gtrsim R^{1/2} L^{-1/3}.$$
On the other hand, by (\ref{discrepancy upper bound}) and Claim \ref{Fab lower bound},
\begin{align*}
|F_{ab}| \cdot |e^{i\tau} - 1| &= |e^{i(\omega_{ab} - \omega_{ab'})} F_{ab} - e^{i(\omega_{a' b} - \omega_{a' b'}) F_{ab}}| \\
&\leq |f_{ab} - f_{ab'}| + |f_{a' b} - f_{a' b'}| + |F_{ab'} - F_{a'b'}| + |F_{ab} - F_{a' b}| \\
&\lesssim \left[\varepsilon^{-1} + \theta^{-1} L^{-2/3}\right] \sqrt R.
\end{align*}
In particular if $L \gg_\theta \varepsilon^{-1/3}$ then we can take $\varepsilon_1 := \varepsilon^2$.
\end{proof}



\printbibliography

\end{document}
