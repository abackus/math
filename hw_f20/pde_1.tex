
% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------

\documentclass[10pt]{article}

\usepackage[margin=.7in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{enumitem}
\usepackage{tikz-cd}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{algorithm2e}
\usepackage{verse,stmaryrd}
\usepackage{fancyvrb}

% Number systems
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\PP}{\mathbb P}
\newcommand{\FF}{\mathbb F}
\newcommand{\DD}{\mathbb D}
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\ch}{\operatorname{ch}}
\newcommand{\Con}{\operatorname{Con}}
\newcommand{\coker}{\operatorname{coker}}
\newcommand{\CVect}{\CC\operatorname{-Vect}}
\newcommand{\Cantor}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\card}{\operatorname{card}}
\newcommand{\dbar}{\overline \partial}
\newcommand{\diam}{\operatorname{diam}}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\End}{\operatorname{End}}
\DeclareMathOperator*{\esssup}{ess\,sup}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\Hess}{\operatorname{Hess}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\Ind}{\operatorname{Ind}}
\newcommand{\Inn}{\operatorname{Inn}}
\newcommand{\interior}{\operatorname{int}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\mesh}{\operatorname{mesh}}
\newcommand{\LL}{\mathcal L_0}
\newcommand{\Leb}{\mathcal{L}_{\text{loc}}^2}
\newcommand{\Lip}{\operatorname{Lip}}
\newcommand{\ppGL}{\operatorname{PGL}}
\newcommand{\ppic}{\vspace{35mm}}
\newcommand{\ppset}{\mathcal{P}}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator*{\Res}{Res}
\newcommand{\Riem}{\mathcal{R}}
\newcommand{\RVect}{\RR\operatorname{-Vect}}
\newcommand{\Sch}{\mathcal{S}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\spn}{\operatorname{span}}
\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\TT}{\mathcal T}
\DeclareMathOperator{\tr}{tr}

% Calculus of variations
\DeclareMathOperator{\pp}{\mathbf p}
\DeclareMathOperator{\zz}{\mathbf z}
\DeclareMathOperator{\uu}{\mathbf u}
\DeclareMathOperator{\vv}{\mathbf v}
\DeclareMathOperator{\ww}{\mathbf w}

% Categories
\newcommand{\Ab}{\mathbf{Ab}}
\newcommand{\Cat}{\mathbf{Cat}}
\newcommand{\Group}{\mathbf{Group}}
\newcommand{\Module}{\mathbf{Module}}
\newcommand{\Set}{\mathbf{Set}}
\DeclareMathOperator{\Fun}{Fun}
\DeclareMathOperator{\Iso}{Iso}

% Complex analysis
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Logic
\renewcommand{\iff}{\leftrightarrow}
\newcommand{\Henkin}{\operatorname{Henk}}
\newcommand{\PA}{\mathbf{PA}}
\DeclareMathOperator{\proves}{\vdash}

% Group
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Out}{Out}

% Other symbols
\newcommand{\heart}{\ensuremath\heartsuit}

\DeclareMathOperator{\atanh}{atanh}

\def\Xint#1{\mathchoice
{\XXint\displaystyle\textstyle{#1}}%
{\XXint\textstyle\scriptstyle{#1}}%
{\XXint\scriptstyle\scriptscriptstyle{#1}}%
{\XXint\scriptscriptstyle\scriptscriptstyle{#1}}%
\!\int}
\def\XXint#1#2#3{{\setbox0=\hbox{$#1{#2#3}{\int}$ }
\vcenter{\hbox{$#2#3$ }}\kern-.6\wd0}}
\def\ddashint{\Xint=}
\def\dashint{\Xint-}

\def\defeq{\overset{\mathrm{def}}{=}}

% Theorems
\theoremstyle{definition}
\newtheorem*{corollary}{Corollary}
\newtheorem*{falselemma}{Grader's ``Lemma"}
\newtheorem{exer}{Exercise}
\newtheorem{lemma}{Lemma}[exer]
\newtheorem{theorem}[lemma]{Theorem}



\usepackage[backend=bibtex,style=alphabetic,maxcitenames=50,maxnames=50]{biblatex}
\renewbibmacro{in:}{}
\DeclareFieldFormat{pages}{#1}

\begin{document}
\noindent
\large\textbf{PDE, HW 1} \hfill \textbf{Aidan Backus} \\

% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------\

\begin{exer}[1.1]
Classify all equations in Chapter 1 as linear, semilinear, quasilinear, or totally nonlinear.
\end{exer}

All of the equations in 1.2.1a and 1.2.2a are easily seen to be linear.

The eikonal equation is totally nonlinear.

The nonlinear Poisson equation is semilinear, since modulo lower-order operators it is the Laplace equation.

The $p$-Laplace equation and the minimal surface equations are totally nonlinear; they both are defined in terms of the divergence of a nonlinear function of the derivatives of $u$.

The Monge-Amp\'ere equation is totally nonlinear because it is defined in terms of the determinant (a nonlinear function) of the derivatives of $u$.

The Hamilton-Jacobi equation is totally nonlinear since it depends on a nonlinear function of the top-order spatial derivative $\nabla u$.

The scalar conservation law is totally nonlinear because it depends on the divergence of a nonlinear function of $u$.

The inviscid Burgers equation is quasilinear because its top-order term in space is $u\partial_x u$.

The scalar reaction-diffusion equation is semilinear because modulo lower-order operators it is the heat equation.

The porous medium equation is totally nonlinear because its top-order term is the Laplacian of $u^\gamma$.

The nonlinear wave equation is semilinear because modulo lower-order operators it is the wave equation.

The KdV equation is semilinear because modulo lower-order operators it is the linear ODE $\partial^3 u(t, \cdot) = 0$.

The nonlinear Schr\"odinger equation is semilinear because modulo lower-operator operators it is the Schr\"odinger equation.

The system of conservation laws is totally nonlinear because it depends on the divergence of a nonlinear function of $u$.

The reaction-diffusion system is semilinear because modulo lower-order operators it is a decoupled system of heat equations.

The Euler equations are quasilinear because their top-order term in space is $\langle u, \nabla u\rangle$.

The Navier-Stokes equations are semilinear because modulo lower-order term they are a decoupled system of Laplace equations.


\begin{exer}[1.5]
Prove the Taylor approximation
\begin{equation}
\label{Taylor}
f(x) = \sum_{|\alpha| \leq k} \frac{1}{\alpha!} \partial^\alpha f(0) x^\alpha + O(|x|^{k+1})
\end{equation}
for $|x|$ small.
\end{exer}

We prove this by induction on $k$. For $k = 0$, the claimed formula reads
\begin{equation}
\label{Taylor base case}
f(x) - f(0) = O(|x|)\end{equation}
which follows from differentability of $f$: writing $x$ in polar coordinates $x = (r, \Theta)$ we have for any $\Theta$
$$\partial_r f(0) = \lim_{\varepsilon \to 0} \frac{f(\varepsilon, \Theta) - f(0)}{\varepsilon}$$
so if $\varepsilon$ is small enough then
$$|f(\varepsilon, \Theta) - f(0)| \leq 2\varepsilon|\partial_r f(0)| \lesssim \varepsilon$$
which is equivalent to (\ref{Taylor base case}), taking $x = (\varepsilon, \Theta)$.

Now assume that (\ref{Taylor}) holds for $k$, fix $x$, and write
$$g(r) = f(rx).$$
Then $g$ is a function of one variable. By our inductive hypothesis applied to $g'$,
$$g'(r) = \sum_{j=0}^k \frac{1}{j!} g^{(j+1)}(0) r^\alpha + O(r^{k+1})$$
so
\begin{align*}
g(r) &= \int_0^r g'(s)~ds = \int_0^r \sum_{j=0}^k \frac{1}{j!} g^{(j+1)}(0) s^j + O(s^{k+1})~ds\\
& = \sum_{j=1}^{k+1} \frac{1}{(j-1)!} g^{(j)}(0) \int_0^r s^{j-1}O(s^{k+1})~ds
\\&= g(0) + \sum_{j=1}^{k+1} \frac{1}{j!} g^{(j)}(0) r^j + O(r^{k+2}) = \sum_{j=0}^{k+1} \frac{1}{j!} g^{(j)}(0) r^j + O(r^{k+2})
\end{align*}
On the other hand, the chain rule gives
$$g^{(j)}(r) = \frac{d^j}{dr^j} f(rx) = \sum_{|\alpha| = j} \frac{j!}{\alpha!} \partial^\alpha f(rx) |x|^\alpha.$$
Therefore
$$g(r) = \sum_{j=0}^{k+1} \sum_{|\alpha| = j} \frac{1}{\alpha!} \partial^\alpha f(rx) |x|^\alpha + O(r^{k+2}|x|^{k+2}).$$
But $f(x) = g(1)$, so
$$f(x) = \sum_{|\alpha| \leq k+1} \frac{1}{\alpha!} \partial^\alpha f(x) |x|^\alpha + O(|x|^{k + 2})$$
which allows us to continue the induction.

\begin{exer}[2.1]
Solve the equation
\begin{equation}
\label{damped transport equation}
\frac{\partial u}{\partial t} + \langle b, \nabla u\rangle + cu = 0
\end{equation}
with Cauchy data $u = g$.
\end{exer}

Let us write $\nabla^\heart$ for the spacetime gradient and $b^\heart = (1, b)$. Then (\ref{damped transport equation}) says that
$$\langle b^\heart, \nabla^\heart u\rangle = -cu,$$
and along the line $\lambda b^\heart$, $\lambda \in \RR$, in spacetime, this is just an ODE; namely, if $(t, x)$ is fixed and we set
$$v(\lambda) = u((t, x) + \lambda b^\heart) = u(t + \lambda, x + \lambda b)$$
then the equation is the exponential decay equation
\begin{equation}
\label{exponential equation}
v'(\lambda) = -cv(\lambda).
\end{equation}
To check this, note that
$$v'(\lambda) = \frac{\partial}{\partial \lambda} u((t, x) + \lambda b^\heart) = \langle b^\heart, \nabla^\heart u((t,x) + \lambda b^\heart)\rangle = -cu((t,x) + \lambda b^\heart) = -cv(\lambda).$$
Solving the equation (\ref{exponential equation}) we get
$$v(\lambda) = e^{-c\lambda}v(0)$$
or in other words
$$u(t + \lambda, x + \lambda b) = e^{-c\lambda} u(t, x).$$
Plugging in the Cauchy data and setting $\lambda = t$ we get
$$u(t, x) = e^{-ct} g(x - tb).$$
This is what one would expect since (\ref{damped transport equation}) is a damped transport equation.

\begin{exer}[2.3]
Show that if $n \geq 3$, $-\Delta u = f$ on $B^0(0, r)$, $u = g$ on $\partial B(0, r)$, then
$$u(0) = \dashint_{\partial B(0, r)} g ~dS + \frac{1}{n(n-2)\alpha(n)} \int_{B(0,r)} \left(\frac{1}{|x|^{n-2}} - \frac{1}{r^{n-2}}\right) f(x)~dx.$$
\end{exer}

I will assume that $f$ is meant to have enough regularity for the below computation to make sense (I think $f$ needs to be at least continuous but definitely needs to be at least $\in L^\infty_{loc}$). Otherwise this problem doesn't make much sense.

Let
$$\varphi(s) = \dashint_{\partial B(0, s)} u~dS = \dashint_{\partial B(0, 1)} u(sy)~dS(y).$$
Then
\begin{align*}
  \varphi'(s) &= \dashint_{\partial B(0, 1)} \langle \nabla u(sx), x\rangle ~dS(x)
  = \dashint_{\partial B(0, s)} \left\langle \nabla u(x), \frac{x}{s} \right\rangle ~dS(x)\\
  &= \frac{1}{n\alpha(n)s^{n-1}} \int_{\partial B(0, s)} \langle \nabla u, \nu\rangle ~dS
  = \frac{1}{n\alpha(n)s^{n-1}} \int_{B(0, s)} \Delta u(x)~dx\\
  &= -\frac{1}{n\alpha(n)s^{n-1}}\int_{B(0, s)} f(x)~dx
\end{align*}
where $\nu$ is the unit outwards normal and we used the divergence theorem. Now
$$\varphi(r) = \dashint_{\partial B(0, r)} u~dS = \dashint_{\partial B(0, r)} g~dS,$$
and
$$\varphi(0) = \varphi(r) - \int_0^r \varphi'(s)~ds = \dashint_{\partial B(0, r)} g~dS + \int_0^r \frac{1}{n\alpha(n)s^{n-1}}\int_{B(0, s)} f(x)~dxds.$$
The problem is reduced to showing that
\begin{equation}
\label{MVP reduction 1}
\int_0^r \frac{1}{n\alpha(n)s^{n-1}}\int_{B(0, s)} f(x)~dxds = \frac{1}{n(n-2)\alpha(n)} \int_{B(0, r)} \left(\frac{1}{|x|^{n-2}} - \frac{1}{r^{n-2}}\right) f(x)~dx.
\end{equation}
Simplifying and letting
$$\psi(s) = \int_{B(0, s)} f(x) ~dx$$
we rather require that
$$\int_0^r s^{1-n} \psi(s) ~ds = \frac{1}{n-2} \int_{B(0, r)} (|x|^{2-n} - r^{2-n}) f(x)~dx.$$

We want to integrate by parts in $s$, so we use the moving-region formula with velocity $\nu$ to compute
$$\psi'(s) = \frac{d}{ds} \int_{B(0, s)} f(x)~dx = \int_{\partial B(0, s)} f\langle \nu,\nu\rangle ~dS = \int_{\partial B(0, s)} f~dS.$$
Let $dv(s) = s^{1-n}~ds$, thus $(2-n)v(s) = s^{2-n}$. Fix $\varepsilon > 0$. Integrating by parts,
\begin{align*}\int_\varepsilon^r s^{1-n} \psi(s) ~ds &= \int_\varepsilon^r \psi~dv = \psi(r) v(r) - \psi(\varepsilon) v(\varepsilon) - \int_\varepsilon^r v~d\psi \\
&= \psi(r) \frac{r^{2-n}}{2-n} - \psi(\varepsilon) \frac{\varepsilon^{2-n}}{2-n} - \int_\varepsilon^r \frac{s^{2-n}}{2-n} \int_{\partial B(0, s)} f~dS ~ds.
\end{align*}
Now $f \in L^\infty(B(0, s))$ for any $s$ sufficiently small, and if we fix such an $s$ and stipulate that $\varepsilon < s$ then
$$|\varepsilon^{2-n} \psi(\varepsilon)| = \varepsilon^{2-n} \left|\int_{B(0, \varepsilon)} f\right| \lesssim \varepsilon^{2-n} ||f||_{L^\infty(B(0, s))} \varepsilon^n \lesssim \varepsilon^2.$$
Taking $\varepsilon \to 0$ and using the fact that $\varphi$ is differentiable and hence continuous, we obtain
$$\int_0^r s^{1-n} \psi(s)~ds = \int_0^r \frac{s^{2-n}}{n-2} \int_{\partial B(0, s)}f~dS~ds -\frac{r^{2-n}}{n-2} \int_{B(0, r)} f(x)~dx.$$
Plugging everything into (\ref{MVP reduction 1}) and simplifying, the problem is reduced to showing that
\begin{equation}
\label{MVP reduction 2}
\int_0^r s^{2-n} \int_{\partial B(0, s)}f~dS~ds = \int_{B(0, r)} |x|^{2-n} f(x)~dx.
\end{equation}
But this follows by writing the right-hand side of (\ref{MVP reduction 2}) in polar coordinates; indeed,
$$\int_{B(0, r)} |x|^{2-n} f(x)~dx = \int_0^r \int_{\partial B(0, s)} |x|^{2-n} f(x)~dS(x) ~ds = \int_0^r s^{2-n} \int_{\partial B(0, s)} f~dS~ds.$$

\begin{exer}[2.4]
Prove that if $U$ is open and precompact in $\RR^d$, $u \in C^2(U) \cap C^0(\overline U)$, $\Delta u = 0$, then
$$\max_{\overline U} u = \max_{\partial U} u.$$
\end{exer}

Let $(e_1, \dots, e_d)$ be the standard basis of $\RR^d$.
\begin{lemma}
\label{rotation invariant}
$\Delta$ is invariant under rotation.
\end{lemma}
\begin{proof}
Let $A = (a_{ij})_{ij} \in O(d)$, the orthogonal group acting on $\RR^d$, and let $f_i = Ae_i$. Then
$$\frac{\partial}{\partial e_i} = \sum_{j=1}^d \frac{\partial f_i}{\partial e_i}\frac{\partial}{\partial f_i} = \sum_{j=1}^d a_{ji} \frac{\partial}{\partial f_j}.$$
So
$$\frac{\partial^2}{\partial^2 e_i} = \sum_{j,k=1}^d a_{ji} a_{ki} \frac{\partial^2}{\partial f_j \partial f_k}.$$
Since $A \in O(d)$, thus $A^t A = 1$, $\sum_i a_{ji} a_{ki} = \delta_j^k$, where $\delta$ is Kronecker's symbol.
So
$$\Delta = \sum_{i=1}^d \frac{\partial^2}{\partial e_i^2} = \sum_{i,j,k=1}^d a_{ji} a_{ki} \frac{\partial^2}{\partial f_j \partial f_k} = \sum_{j,k=1}^d \delta_j^k \frac{\partial^2}{\partial f_j \partial f_k} = \sum_{j=1}^d \frac{\partial^2}{\partial f_j^2}$$
whence $\Delta = A_*\Delta$ where $A_*$ is the pushforward map $A_*Tu(x) = Tu(Ax)$. So $\Delta$ is rotation-invariant.
\end{proof}

Let
$$u_\varepsilon(x) = u(x) + \varepsilon|x|^2.$$
\begin{lemma}
$u_\varepsilon$ has no maximum in $U$.
\end{lemma}
\begin{proof}
Suppose towards contradiction that $x_0 \in U$ and $u_\varepsilon$ has a maximum at $x_0$.
Since $U$ is open, $x_0$ is then a local maximum of $u_\varepsilon$, so $\nabla u_\varepsilon(x_0) = 0$ and $\Hess u_\varepsilon(x_0)$ is either negative or singular.
Since $u$ is harmonic,
$$\tr \Hess u_\varepsilon = \Delta u_\varepsilon = \varepsilon \Delta (x \mapsto |x|^2) = 2d\varepsilon > 0.$$
Since $u_\varepsilon \in C^2(U)$, $\Hess u_\varepsilon(x_0)$ is self-adjoint, so by the spectral theorem we can find a diagonalization $\Lambda$ of $\Hess u_\varepsilon(x_0)$, and $\Lambda$ has positive trace, so $\Lambda$ is not negative and hence neither is $\Hess u_\varepsilon(x_0)$.
Therefore $\ker \Hess u_\varepsilon(x_0)$ is nontrivial.

After rotating $\RR^d$ and noting that $\Delta u$ and $x \mapsto |x|^2$ are both invariant under rotation by Lemma \ref{rotation invariant}, we might as we assume that $e_1 \in \ker \Hess u_\varepsilon(x_0)$. Then
$$\frac{\partial^2 u}{\partial e_1^2}(x_0) = \frac{\partial^2 u_\varepsilon}{\partial e_1^2}(x_0) = \Hess u_\varepsilon(x_0)(e_1) = 0.$$
On the other hand,
$$0 = \Delta u(x_0) = \frac{\partial^2 u}{\partial e_1^2}(x_0) + \cdots + \frac{\partial^2 u}{\partial e_d^2}(x_0).$$
So
$$\frac{\partial^2 u}{\partial e_2^2}(x_0) + \cdots + \frac{\partial^2 u}{\partial e_d^2}(x_0) = 0.$$
Therefore if we set
$$u^\flat(y_2, \dots, y_d) = u(\langle x_0, e_1\rangle, y_2, \dots, y_d),$$
$\Delta u^\flat(x_0) = 0$.
Running the same argument again, we see that $\ker \Hess u_\varepsilon^\flat(x_0)$ is nontrivial.

But $\Hess u_\varepsilon^\flat(x_0)$ is a linear map $\RR^{d-1} \to \RR^{d-1}$, and we had no hypotheses on $d$ except that $d \geq 1$.
Iterating the map $u \mapsto u^\flat$, we obtain a singular linear map $\RR^0 \to \RR^0$, which is absurd.
\end{proof}

Since $u_\varepsilon$ is continuous, $\overline U$ is compact, and $u_\varepsilon|U$ has no maximum, $u_\varepsilon$ attains its maximum on $\partial U$, say at $x_\varepsilon$.
Let $x = \lim_\varepsilon x_\varepsilon$. Since $u_\varepsilon \to u$ uniformly, $x$ is a maximum of $u$, and $x \in \partial U$ since $\partial U$ is compact. So
$$\max_{\overline U} u = \max_{\partial U} u,$$
which was to be shown.

\begin{exer}[2.5a]
Let $v$ be a subharmonic function on $U$. Show that if $B(x, r) \subseteq U$ then
$$v(x) \leq \dashint_{B(x, r)} v~dV.$$
\end{exer}

Let
$$\varphi(r) = \dashint_{\partial B(x, r)} v~dS.$$
By a now familiar application of the divergence theorem, one has
$$\varphi'(r) = \frac{r}{d} \dashint_{B(x, r)} \Delta v(y)~dy \geq 0$$
thus $\varphi$ is increasing and so $\varphi$ attains its minimum at $0$. So
\begin{align*}
\int_{B(x, r)} u(y)~dy &= \int_0^r \int_{\partial B(x, s)} u~dSds = d\alpha(d) \int_0^r s^{d-1} \dashint_{\partial B(x, s)} u~dSds \\
&= d\alpha(d) \int_0^r s^{d-1} \varphi(s) ~ds \geq d\alpha(d) \int_0^r s^{d-1} \varphi(0) ~ds\\
&= d\alpha(d)u(x) \int_0^r s^{d-1} ~ds = \alpha(d)r^du(x),
\end{align*}
thus
$$\dashint_{B(x, r)} u(y)~dy \geq u(x).$$

\begin{exer}[2.5b]
Show that if $v$ is subharmonic then
$$\max_{\overline U} v = \max_{\partial U} v.$$
\end{exer}

Suppose that there is a $x_0 \in U$ which is a maximum of $v$. Then if $r \in (0, d(x_0, \partial U))$,
$$\max_{\overline U} v = v(x_0) \leq \dashint_{B(x_0, r)} v(x)~dx \leq \max_{\overline U} v.$$
Therefore the inequalities collapse, so if $x \in B(x_0, r)$, then $v(x) = v(x_0)$.
So $\{x \in \overline U: v(x) = v(x_0)\}$ is open, but it is also closed because it is the preimage of a point.
Therefore $v$ is constant on connected components of $\overline U$. As already discussed in class this implies the claim.

\begin{exer}[2.5c]
Let $\varphi$ be smooth and convex, $u$ harmonic, and $v = \varphi \circ u$. Show that $v$ is subharmonic.
\end{exer}

One has
\begin{align*}
\Delta v &= \langle \nabla, \nabla v\rangle = \langle \nabla, \varphi' \circ u + \nabla u\rangle \\
&= \langle \nabla, \varphi' \circ u\rangle
= \varphi'' \circ u \langle \nabla u, \nabla u\rangle\\
&\geq |\nabla u|^2 \geq 0.
\end{align*}

\begin{exer}[2.5d]
Let $u$ be harmonic, $v = |\nabla u|^2$. Show that $v$ is subharmonic.
\end{exer}

One has
\begin{align*}\Delta v &= \sum_{i,j=1}^d \frac{\partial^2}{\partial e_i^2} \left(\frac{\partial u}{\partial e_j} \right)^2 = 2\sum_{i,j=1}^d \frac{\partial}{\partial e_i}\left(\frac{\partial u}{\partial e_j} \frac{\partial^2 u}{\partial e_i \partial e_j}\right)\\
&= 2\sum_{i,j=1}^d \frac{\partial^2 u}{\partial e_i \partial e_j} \frac{\partial^2 u}{\partial e_i \partial e_j} + \frac{\partial u}{\partial e_j} \frac{\partial^3 u}{\partial e_i \partial e_j^2}\\
&= 2\sum_{i=1}^d \frac{\partial}{\partial e_i} \Delta u + 2\sum_{i,j=1}^d \left(\frac{\partial^2 u}{\partial e_i \partial e_j}\right)^2\\
&\geq 2\sum_{i=1}^d \frac{\partial}{\partial e_i} \Delta u = 0.
\end{align*}

\begin{exer}[2.7 - Harnack inequality]
Show that if $u \geq 0$ and $\Delta u = 0$ on $B(0, r)$ then
$$r^{n-2} \frac{r - |x|}{(r + |x|)^{n-1}} u(0) \leq u(x) \leq r^{n-2} \frac{r + |x|}{(r - |x|)^{n-1}} u(0).$$
\end{exer}

As stated in Evans, the Poisson formula for the ball $B(0, r)$ is
$$u(x) = \frac{r^2 - |x|^2}{n\alpha(n)r} \int_{\partial B(0, r)} \frac{u(y)}{|x - y|^n} ~dS(y).$$
Since $\partial B(0, r)$ has surface area $n\alpha(n)r^{n-1}$, one has
$$u(x) = r^{n-2} \dashint_{\partial B(0, r)} \frac{r^2 - |x|^2}{|x - y|^n} u(y)~dS(y).$$
If $|y| = r$ and $0 < |x| < r$ then a little inequality bashing, which requires the nonnegativity of $u$, shows
$$\frac{r^2 - |x|^2}{r(r+|x|)^n} \leq \frac{r^2 - |x|^2}{|x - y|^n} \leq \frac{r^2 - |x|^2}{r(r-|x|)^n}.$$
This implies
$$\frac{r-|x|}{r(r+|x|)^{n-1}} \leq \frac{r^2 - |x|^2}{|x - y|^n} \leq \frac{r+|x|}{r(r-|x|)^{n-1}}$$
and hence
$$r^{n-2} \dashint_{\partial B(0, r)} \frac{r-|x|}{r(r+|x|)^{n-1}} u(y)~dS(y) \leq u(x) \leq r^{n-2} \dashint_{\partial B(0, r)} \frac{r+|x|}{r(r-|x|)^{n-1}} u(y)~dS(y)$$
but, by the mean-value principle,
$$r^{n-2} \frac{r-|x|}{r(r+|x|)^{n-1}} u(0) \leq u(x) \leq r^{n-2} \frac{r+|x|}{r(r-|x|)^{n-1}} u(0)$$
which is what was claimed.

\begin{exer}[2.10a - Schwarz reflection]
Let $U^+$ be the open half-ball in $\RR^n$. Let $u \in C^2(U^+)$ be harmonic, $u(x_1, \dots, x_{n-1}, 0) = 0$ for all $x$, and extend $u$ to the open ball $U$ by
$$u(x) = -u(x_1, \dots, x_{n-1}, -x_n)$$
if $x_n < 0$.
Show that $u \in C^2(U)$ is harmonic.
\end{exer}

We first note that if we define $y$ by $y_i = x_i$ for $i \leq n-1$, $y_n = -x_n$, then
$$\frac{\partial^2 u}{\partial x_i^2} = \frac{\partial^2 u}{\partial y_i^2}$$
for any $i$ since $(\partial y_i/\partial x_i)^2 = 1$; so
$$\Delta u(x_1, \dots, x_n) = \Delta(x_1, \dots, x_{n-1}, -x_n)$$
and so $u$ is harmonic provided that it is $C^2$ bear the plane $x_n = 0$.
It suffices to show that $u$ satisfies the mean-value property there; namely,
$$0 = \dashint_{\partial B(x^*, 0, r)} u~dS$$
for any $r$ sufficiently small and any $x^* \in \RR^{n-1}$ with $|x^*| < 1$.
(Indeed, we showed that the mollification of any function which satisfies the mean-value property is actually equal to the function, so $u$ is automatically smooth and thus harmonic.)
But
$$\int_{\partial B(x^*, 0, r)} u~dS = \int_{\partial B(x^*, 0, r)^+} u~dS + \int_{\partial B(x^*, 0, r)^-} u~dS = \int_{\partial B(x^*, 0, r)^+} u~dS - \int_{\partial B(x^*, 0, r)^+} u~dS = 0.$$
Here $\partial B(x^*, 0, r)^\pm$ denotes those elements $(y_1, \dots, y_n) \in \partial B(x^*, 0, r)$ with $\pm y_n > 0$.

\begin{exer}[2.11]
Let $\overline u$ denote the Kelvin transform of $u$. Show that if $u$ is harmonic then so is $\overline u$.
\end{exer}

Per the hint, we first show:

\begin{lemma}
Let $\overline x = x/|x|^2$. Then
$$\frac{\partial \overline x}{\partial x} \left(\frac{\partial \overline x}{\partial x}\right)^t = |\overline x|^4.$$
\end{lemma}
\begin{proof}
This is a straightforward computation. One has
$$\left(\frac{\partial \overline x}{\partial x}\right)_{ij} = \frac{\partial \overline x_i}{\partial x_j} = \frac{\delta_i^j}{|x|^2} - 2\frac{x_ix_j}{|x|^4}.$$
Thus
\begin{align*}\left(\frac{\partial \overline x}{\partial x} \left(\frac{\partial \overline x}{\partial x}\right)^t\right)_{ij} &= \sum_k \left(\frac{\partial \overline x}{\partial x}\right)_{ik} \left(\frac{\partial \overline x}{\partial x}\right)_{jk}\\
& = \sum_k \left(\frac{\delta_i^j}{|x|^2} - 2\frac{x_ix_k}{|x|^4}\right)\left(\frac{\delta_j^k}{|x|^2} - 2\frac{x_jx_k}{|x|^4}\right)\\
&= \frac{\delta_i^j}{|x|^2} - 4\frac{x_ix_j}{|x|^6} + 4\frac{x_ix_j}{|x|^8}|x|^2 \\
&= \frac{\delta_i^j}{|x|^2}.
\end{align*}
Moreover, $|\overline x|^4 = |x|^{-2}$, so this is as desired.
\end{proof}

Since
$$\frac{\partial^2\overline x_j}{\partial x_i^2} = -2|x|^{-4}(x_j + 2\delta_i^jx_i) + 8|x|^{-6}x_i^2x_j,$$
one has
$$\Delta \overline x = d(2-d)\frac{x}{|x|^{-4}}.$$
Moreover,
\begin{align*}
\Delta (u(x/|x|^2)|x|^{2-d}) &= \Delta(|x|^{2-d})u(\overline x) + 2 \left\langle \nabla u(x), \frac{\partial \overline x}{\partial x} \frac{\partial |x|^{2-d}}{\partial x}\right\rangle \\
&\qquad+ |x|^{2-d}\langle \nabla u(x), \Delta \overline x\rangle + |x|^{2-d} \tr \left(\left(\frac{\partial \overline x}{\partial x}\right)^t \Hess u(x) \frac{\partial \overline x}{\partial x}\right).
\end{align*}
Up to scaling, $x \mapsto |x|^{2-d}$ is the fundamental solution of $\Delta$, so $\Delta(|x|^{2-d})u(\overline x) = 0$.
By the cyclic permutation property of trace and the previous lemma,
\begin{align*}\tr \left(\left(\frac{\partial \overline x}{\partial x}\right)^t \Hess u \frac{\partial \overline x}{\partial x}\right) &= \tr \left(\frac{\partial \overline x}{\partial x} \left(\frac{\partial \overline x}{\partial x}\right)^t \Hess u(x)\right) \\
&= d|\overline x|^4 \tr \Hess u(x) = d|\overline x|^4 \Delta u(x) = 0.
\end{align*}
It remains to show that
$$2 \left\langle \nabla u(x), \frac{\partial \overline x}{\partial x} \frac{\partial |x|^{2-d}}{\partial x}\right\rangle+ |x|^{2-d}\langle \nabla u(x), \Delta \overline x\rangle = 0.$$
In fact,
\begin{align*}\frac{\partial |x|^{2-d}}{\partial x} &= |x|^{-2}\left(1 - 2\frac{x \otimes x}{|x|^2}\right)(2-d)|x|^{-d}x\\
&= 2(d-2)|x|^{-2-d}\langle \nabla u(x), x\rangle = - 2|x|^{2-d}\langle \nabla u(x), \Delta \overline x\rangle
\end{align*}
which implies the claim.


\begin{exer}[Extra 1]
Let $d \geq 4$, $u \in C^2(\RR^d)$, $\Delta u = g$, $g \in C^2_c(\RR^d)$. Show that if $u(x) = O(|x|^{1-d})$ then $\int g = 0$.
\end{exer}

Suppose that $g \in C_2^c(B(0, R))$. Then by the divergence theorem,
$$\int_{\RR^d} g(x)~dx = \int_{B(0, R)} \Delta u(x)~dx = \int_{\partial B(0, R)} \frac{\partial u}{\partial \nu}~dS.$$
We now apply the Kelvin transform, which maps $\overline B(0, R)^c$ to $B(0, R^{-1})$.
In a neighborhood of $\overline B(0, R)^c$, $u$ is harmonic, so $\overline u$ is harmonic on $B(0, R^{-1})$.
Moreover, if $x \in \partial B(0, R)$ then $\overline x = R^{-2}x$ so the Jacobian of $x \mapsto \overline x$ restricted to $\partial B(0, R)$ is $R^{-2d}$. So by the divergence theorem again,
\begin{align*}\int_{\partial B(0, R)} \frac{\partial u}{\partial \nu}~dS& = R^{2d} \int_{\partial B(0, R^{-1})} \frac{\partial u}{\partial \nu}(\overline x)~dS(x) = R^{2d(2-d)} \int_{\partial B(0, R^{-1})} \frac{\partial \overline u}{\partial \nu}~dS\\&
= R^{2d(2-d)} \int_{B(0, R^{-1})} \Delta \overline u(x)~dx = 0.
\end{align*}

\begin{exer}[Extra 2]
Let $g \in C^2_c(\RR^3)$ and
$$u(x) = \frac{1}{4\pi} \int_{\RR^3} g(x-y) \frac{e^{-|y|}}{|y|}~dy.$$
Show that $u \in C^2(\RR^3)$, that $u(x) = O(|x|^{-\infty})$, and $u - \Delta u = g$.
\end{exer}

\begin{theorem}
$u \in C^2(\RR^3)$.
\end{theorem}
\begin{proof}
One has
$$u(x) = \frac{1}{4\pi} \int_{B(0, \varepsilon)} g(x - y) \frac{e^{-|y|}}{|y|}~dy + \frac{1}{4\pi} \int_{\RR^3 \setminus B(0, \varepsilon)} g(x - y) \frac{e^{-|y|}}{|y|}~dy$$
and
\begin{align*}
\int_{B(0, \varepsilon)} g(x - y) \frac{e^{-|y|}}{|y|}~dy
&= \int_0^\varepsilon \int_{\partial B(0, s)} g(x - y) \frac{e^{-|y|}}{|y|} ~dS(y) ~ds\\
&= \int_0^\varepsilon \frac{e^{-s}}{s} \int_{\partial B(0, s)} g(x - y) ~dS(y) ~ds
\end{align*}
so
$$\left|\int_{B(0, \varepsilon)} g(x - y) \frac{e^{-|y|}}{|y|}~dy\right| \leq ||g||_{L^\infty(\RR^3)} \int_0^\varepsilon \frac{e^{-s}}{s} 4\pi s^2~ds \lesssim \varepsilon.$$
The implied constant here only depends on $||g||_{L^\infty}$; in particular, it is independent of $x$, so
$$u(x) = \lim_{\varepsilon \to 0} \frac{1}{4\pi} \int_{\RR^3 \setminus B(0, \varepsilon)} g(x - y) \frac{e^{-|y|}}{|y|}~dy \defeq \lim_{\varepsilon \to 0} u_\varepsilon(x)$$
uniformly.

The integral $u_\varepsilon(x)$ has a $C^2$ integrand in $x$ and a continuous integrand in the variable of integration $y$, so we can commute the differential operator $\partial^\alpha$, $|\alpha| \leq 2$, with the integral sign and conclude that
$$\partial^\alpha u_\varepsilon(x) = \frac{1}{4\pi} \int_{\RR^3 \setminus B(0, \varepsilon)} \partial^\alpha_x g(x - y) \frac{e^{-|y|}}{|y|}~dy.$$
Moreover, if $0 < \varepsilon_1 < \varepsilon_2$ then
\begin{align*}|\partial^\alpha u_{\varepsilon_1}(x) - \partial_\alpha u_{\varepsilon_2}(x)| &= \left|\frac{1}{4\pi} \int_{B(0, \varepsilon_2) \setminus B(0, \varepsilon_1)} \partial^\alpha_x g(x - y) \frac{e^{-|y|}}{|y|}~dy\right|\\
&\leq \frac{||g||_{W^{2,\infty}}}{4\pi} \int_{\varepsilon_1}^{\varepsilon_2} \frac{e^{-s}}{s} \int_{\partial B(0, s)} ~dSds\\
&\lesssim \varepsilon_2
\end{align*}
where the implied constant only implies on the Sobolev norm
$$||g||_{W^{2, \infty}} \defeq \sum_{|\alpha| \leq 2} ||\partial^\alpha g||_{L^\infty}$$
and not $x$, so the derivatives $\partial^\alpha u_\varepsilon$ are uniformly Cauchy in $\varepsilon$.
Therefore they converge to $\partial^\alpha u$. In particular,
$$\partial^\alpha u(x) = \lim_{\varepsilon \to 0} \frac{1}{4\pi} \int_{\RR^3 \setminus B(0, \varepsilon)} \partial^\alpha_x g(x - y) \frac{e^{-|y|}}{|y|}~dy$$
and $u \in C^2(\RR^3)$.
\end{proof}

\begin{theorem}
One has $u(x) = O(|x|^{-\infty})$.
\end{theorem}
\begin{proof}
It suffices to check this for $u_\varepsilon$ as long as the sequence of implied constants in the relation $u_\varepsilon(x) = O(|x|^{-\infty})$ do not depend on $\varepsilon$.
In fact
\begin{align*}
|u_\varepsilon(x)| &= \frac{1}{4\pi} \left|\int_{\RR^3 \setminus B(0, \varepsilon)} g(x - y) \frac{e^{-|y|}}{|y|}~dy\right| = \frac{1}{4\pi} \left|\int_{\RR^3 \setminus B(x, \varepsilon)} g(y) \frac{e^{-|x - y|}}{|x - y|}~dy\right| \\
&= \frac{1}{4\pi} \left|\int_{\supp g \setminus B(x, \varepsilon)} g(y) \frac{e^{-|x - y|}}{|x - y|}~dy\right|
\leq \frac{||g||_{L^\infty}}{4\pi} \int_{\supp g \setminus B(x, \varepsilon)}  \frac{e^{-|x - y|}}{|x - y|}~dy\\
&\lesssim \int_{\supp g-x} \frac{e^{-|y|}}{|y|}~dy
\end{align*}
where the implied constants only depend on $||g||_{L^\infty}$ and the last inequality is valid as long as $x$ is large enough, since $\supp g$ is compact.

Since $\supp g$ is compact, we may select $R > 0$ such that $\supp g \subseteq B(0, R)$.
If $y \in \supp g - x$ then $x + y \in \supp g$ so
$$||x| - |y|| \leq |x + y| \leq R$$
so if $|x| > R$ we have $|y| \geq |x| - R$. Therefore
$$\supp g -x \subseteq \RR^3 \setminus B(0, |x| - R).$$
So
\begin{align*}
|u_\varepsilon(x)| &\lesssim \int_{\supp g-x} \frac{e^{-|y|}}{|y|}~dy \leq \int_{\RR^3 \setminus B(0, |x| - R)} \frac{e^{-|y|}}{|y|}~dy\\
&= \int_{|x|-R}^\infty \frac{e^{-s}}{s} |\partial B(0, s)|~ds \lesssim \int_{|x|-R}^\infty se^{-s}~ds \\
&\lesssim e^{-|x|} = O(|x|^{-\infty})
\end{align*}
and the implied constants only depend on $||g||_{L^\infty}$ and $R$, which in turn only depended on $\supp g$.
\end{proof}

We now compute $(1 - \Delta)u$. We already established that $u_\varepsilon \to u$ in $W^{2,\infty}$, so
\begin{align*}
(1 - \Delta)u(x) &= \lim_{\varepsilon \to 0} \frac{1}{4\pi} \int_{\RR^3 \setminus B(0, \varepsilon)} (1 - \Delta_x) g(x - y) \frac{e^{-|y|}}{|y|}~dy\\
&= -\frac{1}{4\pi} \lim_{\varepsilon \to 0} \int_{\RR^3 \setminus B(x, \varepsilon)} g(y) (1 - \Delta_x) \frac{e^{-|x-y|}}{|x-y|}~dy.
\end{align*}
We now compute $\Delta_x e^{-|x-y|}/|x-y|$ subject to $|x - y| > \varepsilon$.
Let $\eta(x) = |x-y|$. A Mathematica-aided computation gives
$$\Delta \frac{e^{-\eta(x)}}{\eta(x)} = \frac{e^{-\eta(x)}}{\eta(x)^3} \left((\eta(x)^2 + \eta(x) + 2)|\nabla \eta(x)|^2 - (\eta(x)^2 + \eta(x))\Delta \eta(x) \right).$$
Now
$$\nabla \eta(x) = \frac{x-y}{|x-y|}$$
whence $|\nabla \eta(x)|^2 = 1$. Meanwhile,
$$\Delta \eta(x) =\left \langle \nabla_x, \frac{x-y}{|x-y|} \right\rangle = \frac{2}{|x - y|} = 2\eta(x)^{-1}.$$
Therefore
$$\Delta\frac{e^{-\eta(x)}}{\eta(x)} = \frac{e^{-\eta(x)}}{\eta(x)^3}(\eta(x)^2 + \eta(x) + 2 - 2\eta(x) - 2) = e^{-\eta(x)}(\eta(x)^{-1} - \eta(x)^{-2})$$
so
$$(1 - \Delta_x) \frac{e^{-|x-y|}}{|x-y|} = \frac{e^{-|x-y|}}{|x-y|^2}.$$
I guess this is somehow supposed to be an approximation to the identity as $x \to y$, but there's no good way to shrink its support, so here I got stuck.

\begin{exer}[Extra 3]
Let $\Omega = \overline \Omega_0 \cup \overline \Omega_1 \subset \RR^2$ be a bounded open set in the plane, and assume that $\Omega_0 \cap \Omega_1$ is empty, so that $\partial \Omega_0 \cap \partial \Omega_1$ is empty.
Assume that a particle moves in $\Omega$ with uniform probability of moving in any direction at any time.
Show that the probability $p(x)$ of eventually escaping through $\partial \Omega_0$ is the solution to the Dirichlet problem $\Delta p = 0$, $p|\partial \Omega_i = 1 - i$.
\end{exer}

By $\varepsilon \ZZ^2$ we will mean $\{(\varepsilon n, \varepsilon m): n,m \in \NN\}$, which we view as a graph by declaring that points in $\varepsilon \ZZ^2$ are vertices, and there is an edge $(x, y)$ provided that $|x - y| = \varepsilon$. Let $E_\varepsilon$ be the set of edges.
Therefore we may introduce the graph Laplacian
$$\Delta_\varepsilon f(x) \defeq -\varepsilon^{-2} \sum_{(x, y) \in E_\varepsilon} f(x) - f(y)$$
for any function $f: G \to \RR$ where $G \subseteq \varepsilon \ZZ^2$ is a full subgraph.
Here the factor of $\varepsilon^{-2}$ is needed to normalize since edges have length $\varepsilon$ instead of $1$.

Define $p_\varepsilon(x)$ the probability of escape through $\partial \Omega_0$ for the \emph{discrete time} stochastic process wherein a particle starts at position $x \in G_\varepsilon = \varepsilon \ZZ^2 \cap \Omega$, and at time $\varepsilon t$, $t \in \NN$, moves to an adjacent vertex, selected uniformly at random.
Then if $y$ is an adjacent vertex to $x$, the probability that the particle moves to $y$ at time $\varepsilon$ is $1/4$ and the probability that the particle then escapes through $\Omega_0$ is $p(y)/4$. Therefore
$$4p_\varepsilon(x) = \sum_{(x, y) \in E_\varepsilon} f(y),$$
which implies that $\Delta_\varepsilon p_\varepsilon = 0$.

Now one ``clearly" (once Brownian motion has been defined?) has $p(x) = \lim_\varepsilon p_\varepsilon(x_\varepsilon)$ where $x_\varepsilon$ is a vertex of $\varepsilon \ZZ^2$ which best approximates $x$. Since the boundary conditions are similarly clear, it suffices to show that:

\begin{lemma}
Let $f \in C^2(\Omega)$. Then for every $(x, y) \in \Omega$, if $(x_n, y_n) \in 1/n \ZZ^2$ satisfies $\lim_n (x_n, y_n) = (x,y)$ then
$$\Delta f(x, y) = \lim_{n \to \infty} \Delta_{1/n} f(x_n, y_n).$$
\end{lemma}
\begin{proof}
One has
\begin{align*}
\frac{\partial^2 f}{\partial x^2} (x, y) &= \lim_{h \to 0} \frac{f(x - h, y) + f(x + h, y) - 2f(x, y)}{h^2}\\
&= \lim_{n \to \infty} \lim_{h \to \infty} \frac{f(x_n - h, y) + f(x_n + h, y) - 2f(x_n, y)}{h^2}\\
&= \lim_{n \to \infty} n^2(f(x_n - 1/n, y_n) + f(x_n + 1/n, y_n) - 2f(x_n, y_n))
\end{align*}
where we used the fact that $f$ is continuous and hence locally bounded to commute the limits; similarly,
$$\frac{\partial^2 f}{\partial y^2} (x, y) = \lim_{n \to \infty} n^2(f(x_n, y_n - 1/n) + f(x_n, y_n + 1/n) - 2f(x_n, y_n))$$
so that
$$\Delta f(x, y) = -\lim_{n \to \infty} n^2 \sum_{(v_n, x_n, y_n) \in E_{1/n}} f(x_n, y_n) - f(v_n) = \lim_{n \to \infty} \Delta_{1/n} f(x_n, y_n)$$
which was to be shown.
\end{proof}

Then, since the graph Laplacians of $p_\varepsilon$ all vanish, so must the Laplacian of $p$.

\begin{exer}[Extra 4 - Hopf lemma]
Let $\Omega$ be a bounded open set in $\RR^d$.
Let $u \in C^2(\Omega) \cap C^1(\overline \Omega)$ be a nonconstant harmonic function, and let $x_0 \in \partial \Omega$ maximize $u$.
Show that $\partial u/\partial \nu (x_0) > 0$.
\end{exer}

Following the lemma, we break up the proof of Hopf's lemma into several smaller lemmata.

\begin{lemma}
One has $\partial u/\partial \nu (x_0) \geq 0$.
\end{lemma}
\begin{proof}
Suppose not, and let $-\nu$ denote the inward-facing unit normal. Then $\partial u/\partial(-\nu) (x_0) > 0$; that is,
$$\lim_{h \to 0} \frac{u(x_0 - h\nu) - u(x_0)}{h} > 0$$
or in other words $u(x_0 - h\nu) > u(x_0)$ for sufficiently small $h$.
But $x_0 - h\nu \in \Omega$, so this is a contradiction.
\end{proof}

\begin{lemma}
If $d = 1$, then Hopf's lemma holds.
\end{lemma}
\begin{proof}
If $d =1 $ then $\Omega$ is a countable disjoint union of open intervals.
By discarding the intervals which do not touch $x_0$, we can assume that $\Omega = (a, b) \setminus \{x_0\}$.
Since $u$ is harmonic, $u'' = 0$, so $u$ is a line, so it follows that $x_0 = a$ or $x_0 = b$; otherwise the slope of $u$ would change at $x_0$.
Suppose that $x_0 = b$; the other case is similar.
Then $u' > 0$ since $u' \neq 0$ and $u$ is nondecreasing in a neighborhood of $b$, so has nonnegative slope.
In particular $u'(b) > 0$.
\end{proof}

Using the invariance of $\Delta$ under $O(d) \rtimes \RR^d$ (we already showed the $O(d)$ invariance, and the $\RR^d$ invariance is clear), we might as well assume that $x_0 = 0$ and $\partial \Omega$ is tangent to a plane $z = 0$.
Let $y = (r, 0, \dots, 0)$, where $r$ is so small that $B(y, r) \subseteq \Omega$.
Let $C = B(y, r) \setminus B(y, r/2)$ and let
$$v_{\lambda,\varepsilon}(x) = u(x) + \varepsilon \left(e^{-\lambda|x-y|^2} - e^{-\lambda r^2} \right).$$

\begin{lemma}
If $\lambda$ is large enough then $v_{\lambda,\varepsilon}$ is subharmonic on $C$.
\end{lemma}
\begin{proof}
One has
$$\Delta v_{\lambda,\varepsilon}(x) = \varepsilon \Delta_x e^{-\lambda|x-y|^2}$$
since $u$ is harmonic and $e^{-\lambda r^2}$ does not depend on $x$.

One has
$$\frac{\partial}{\partial x_i} \exp(-\lambda |x - y|^2) = - \lambda \exp(-\lambda |x -y|^2) \frac{\partial}{\partial x_i} (x_i - y_i)^2 = -2\lambda\exp(-\lambda|x-y|^2) (x_i - y_i).$$
Differentiating again,
$$\frac{\partial^2}{\partial x_i^2} \exp(-\lambda |x - y|^2) = 4\lambda^2 \exp(-\lambda|x-y|^2)(x_i - y_i)^2 - 2\lambda \exp(-\lambda^2|x-y|^2).$$
Therefore
$$\Delta_x e^{-\lambda|x-y|^2} = (4\lambda^2|x-y|^2 - 2\lambda) \exp(-\lambda|x-y|^2) \geq (r\lambda^2 - 2\lambda) \exp(-\lambda|x-y|^2).$$
Thus the claim follows if $\lambda \geq 2/r$.
\end{proof}

\begin{lemma}
If $\varepsilon$ is small enough and $\lambda$ is large enough, then for every $x \in C$, $v_{\lambda,\varepsilon}(x) \leq u(0)$.
\end{lemma}
\begin{proof}
Suppose towards contradiction that there is a sequence $\varepsilon_n \to 0$ and $y_n \in C$ such that
$$u(0) < v_{\lambda,\varepsilon_n}(y_n).$$
Now let $x_n$ maximize $v_{\lambda,\varepsilon_n}$ on $\overline C$; by the maximum principle we can take $x_n \in \partial C$.
Then $u(0) < v_{\lambda,\varepsilon_n}(x_n)$.
Since $\partial C$ is compact, there is a limit $x_0'$ of $x_n$, and
$$u(0) \leq u(x_0') \leq u(x_0).$$
Therefore $x_0' \in \partial \Omega$ since $x_0'$ maximizes $u$.
Since $\partial C \cap \partial \Omega = \{0\}$, it follows that $0 = x_0'$ and so
$$0 = \lim_{n \to \infty} x_n.$$
In particular, the $x_n$ are not in $\overline{B(y,r/2)}$, but $x_n \in \partial C$, so $x_n \in \partial B(y,r)$; that is, $|x_n - y| = r$.
So
$$u(x_n) = v_{\lambda,\varepsilon_n}(x_n) > u(0).$$
Since $u$ is continuous, it follows that there is a neighborhood $U$ of $x_n$ such that $u|U > u(0)$; but $U \cap \Omega$ is nonempty, so this is a contradiction.
\end{proof}

We are now ready to prove Hopf's lemma. By a previous lemma, it suffices to show that $\partial u/\partial \nu(0) \neq 0$, so assume towards contradiction that
$$\frac{\partial u}{\partial \nu}(0) = 0.$$
Let $g(t) = u(t, 0, \dots, 0)$.
Then
$$0 = \lim_{h \to 0} \frac{g(h) - g(0)}{h}$$
by our contradiction assumption. But if $x = (h, 0, \dots, 0)$ then
\begin{align*}
\varepsilon(e^{-\lambda(r-h)^2} - e^{-\lambda r^2}) &= \varepsilon(e^{-\lambda|x-y|^2} - e^{-\lambda r^2})\\
&\leq v_{\lambda,\varepsilon}(x) - u(x) \leq u(0) - u(x)
= g(0) - g(h).
\end{align*}
Now
$$\lim_{h \to 0} \frac{e^{-\lambda(r-h)^2} - e^{-\lambda r^2}}{h} = \frac{\partial }{\partial r} e^{-\lambda r^2} = -2\lambda re^{-\lambda r^2}.$$
This precludes that $g(0) - g(h) = o(h)$.


\end{document}
