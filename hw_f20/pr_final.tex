
% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------

\documentclass[10pt]{article}

\usepackage[margin=.7in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{enumitem}
\usepackage{tikz-cd}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{algorithm2e}
\usepackage{verse,stmaryrd}
\usepackage{fancyvrb}

% Number systems
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\PP}{\mathbb P}
\newcommand{\FF}{\mathbb F}
\newcommand{\DD}{\mathbb D}
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\ch}{\operatorname{ch}}
\newcommand{\Con}{\operatorname{Con}}
\newcommand{\coker}{\operatorname{coker}}
\newcommand{\CVect}{\CC\operatorname{-Vect}}
\newcommand{\Cantor}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\card}{\operatorname{card}}
\newcommand{\dbar}{\overline \partial}
\newcommand{\diam}{\operatorname{diam}}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\End}{\operatorname{End}}
\DeclareMathOperator*{\esssup}{ess\,sup}
\newcommand{\Hess}{\operatorname{Hess}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\Ind}{\operatorname{Ind}}
\newcommand{\Inn}{\operatorname{Inn}}
\newcommand{\interior}{\operatorname{int}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\mesh}{\operatorname{mesh}}
\newcommand{\LL}{\mathcal L_0}
\newcommand{\Leb}{\mathcal{L}_{\text{loc}}^2}
\newcommand{\Lip}{\operatorname{Lip}}
\newcommand{\ppic}{\vspace{35mm}}
\newcommand{\ppset}{\mathcal{P}}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator*{\Res}{Res}
\newcommand{\Riem}{\mathcal{R}}
\newcommand{\RVect}{\RR\operatorname{-Vect}}
\newcommand{\Sch}{\mathcal{S}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\spn}{\operatorname{span}}
\newcommand{\Spec}{\operatorname{Spec}}
\DeclareMathOperator{\sech}{sech}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\TT}{\mathcal T}
\DeclareMathOperator{\tr}{tr}

% Calculus of variations
\DeclareMathOperator{\pp}{\mathbf p}
\DeclareMathOperator{\zz}{\mathbf z}
\DeclareMathOperator{\uu}{\mathbf u}
\DeclareMathOperator{\vv}{\mathbf v}
\DeclareMathOperator{\ww}{\mathbf w}

% Categories
\newcommand{\Ab}{\mathbf{Ab}}
\newcommand{\Cat}{\mathbf{Cat}}
\newcommand{\Group}{\mathbf{Group}}
\newcommand{\Module}{\mathbf{Module}}
\newcommand{\Set}{\mathbf{Set}}
\DeclareMathOperator{\Fun}{Fun}
\DeclareMathOperator{\Lie}{Lie}
\DeclareMathOperator{\Iso}{Iso}

% Complex analysis
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Logic
\renewcommand{\iff}{\leftrightarrow}
\newcommand{\Henkin}{\operatorname{Henk}}
\newcommand{\PA}{\mathbf{PA}}
\DeclareMathOperator{\proves}{\vdash}

% Group
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Out}{Out}

\DeclareMathOperator{\Diffeo}{Diffeo}

\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\Ad}{Ad}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\ppGL}{\operatorname{PGL}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\SO}{\operatorname{SO}}
\newcommand{\iprod}{\mathbin{\lrcorner}}


% Other symbols
\newcommand{\heart}{\ensuremath\heartsuit}
\newcommand{\club}{\ensuremath\clubsuit}

\DeclareMathOperator{\atanh}{atanh}
\DeclareMathOperator{\codim}{codim}
\DeclareMathOperator{\PD}{PD}

% Theorems
\theoremstyle{definition}
\newtheorem*{corollary}{Corollary}
\newtheorem*{falselemma}{Grader's ``Lemma"}
\newtheorem{exer}{Exercise}
\newtheorem{lemma}{Lemma}[exer]
\newtheorem{theorem}[lemma]{Theorem}

\def\Xint#1{\mathchoice
{\XXint\displaystyle\textstyle{#1}}%
{\XXint\textstyle\scriptstyle{#1}}%
{\XXint\scriptstyle\scriptscriptstyle{#1}}%
{\XXint\scriptscriptstyle\scriptscriptstyle{#1}}%
\!\int}
\def\XXint#1#2#3{{\setbox0=\hbox{$#1{#2#3}{\int}$ }
\vcenter{\hbox{$#2#3$ }}\kern-.6\wd0}}
\def\ddashint{\Xint=}
\def\dashint{\Xint-}

\usepackage[backend=bibtex,style=alphabetic,maxcitenames=50,maxnames=50]{biblatex}
\renewbibmacro{in:}{}
\DeclareFieldFormat{pages}{#1}

\begin{document}
\noindent
\large\textbf{Probability, Final Exam} \hfill \textbf{Aidan Backus} \\

% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------\

\begin{exer}
Compute
$$\lim_{p \to \infty} \int_0^\infty \frac{\sin(x^p)}{x^p}~dx.$$
\end{exer}

We treat the integrals on $[0, 1]$ and $(1, \infty)$ separately. For the latter, the integrands are dominated, at least for $p > 2$, by $x^{-2}$, which is integrable on $(1, \infty)$, so by dominated convergence, the integrals on $(1, \infty)$ converge to $0$.
Meanwhile, $\sin y/y$ is bounded on $[0, 1]$ by $1$, and $1$ is integrable on $[0, 1]$, so the integrals on $[0, 1]$ satisfy
$$\lim_{p \to \infty} \int_0^1 \frac{\sin(x^p)}{x^p} ~dx = \int_0^1 \lim_{p \to \infty} \frac{\sin(x^p)}{x^p}~dx = \int_0^1 ~dx = 1$$
by dominated convergence.
In particular,
$$\lim_{p \to \infty} \int_0^\infty \frac{\sin(x^p)}{x^p}~dx = 1.$$

\begin{exer}
Let $\mu$ be a Borel probability measure on $\RR$ and let $B$ be a Borel set of Lebesgue measure $a$. Compute
$$\int_{-\infty}^\infty \mu(x + B)~dx.$$
\end{exer}

One has
$$\int_{-\infty}^\infty \mu(x+B)~dx = \int_{-\infty}^\infty \int_{x + B} ~d\mu~dx = \int_{-\infty}^\infty \int_{-\infty}^\infty 1_B(y - x) ~d\mu(y)~dx.$$
By Tonelli's theorem,
$$\int_{-\infty}^\infty \int_{-\infty}^\infty 1_B(y - x) ~d\mu(y)~dx = \int_{-\infty}^\infty \int_{-\infty}^\infty 1_B(y - x) ~dx ~d\mu(y).$$
Now by translation invariance, the inner integral does not depend on $y$, and in fact equals $a$. Since $\mu$ is a probability measure, the outer integral then evaluates to $a$.

\begin{exer}
Let $X_n$ be iid exponential random variables of rate $\lambda$. Show that
$$\limsup_{n \to \infty} \frac{X_n}{\log n} = \frac{1}{\lambda}$$
almost surely.
\end{exer}

Let $\varepsilon > 0$.
First we show that
\begin{equation}
\label{Borel Cantelli 1}
\PP\left(\limsup_{n \to \infty}\left(X_n > \frac{\log n}{\lambda} + \varepsilon \log n\right)\right) = 0.
\end{equation}
Indeed,
\begin{align*}
\PP\left(X_n > \frac{\log n}{\lambda} + \varepsilon \log n\right) &= \lambda \int_{(1/\lambda + \varepsilon) \log n}^\infty e^{-\lambda x} ~dx \\
&= \int_{(1 + \varepsilon\lambda) \log n}^\infty e^{-y}~dy = \exp(-(1 + \varepsilon\lambda) \log n)
\\
&= n^{-(1 + \varepsilon\lambda)}
\end{align*}
which is summable as $\varepsilon\lambda > 0$, so (\ref{Borel Cantelli 1}) follows by the Borel-Cantelli lemma. It follows that
$$\limsup_{n \to \infty}\frac{X_n}{\log n} \leq \frac{1}{\lambda}$$
almost surely.

Conversely, we use the converse of the Borel-Cantelli lemma to show that
\begin{equation}
\label{Borel Cantelli 2}
\PP\left(\limsup_{n \to \infty}\left(X_n > \frac{\log n}{\lambda} - \varepsilon \log n\right)\right) = 1.
\end{equation}
Here we use that the $X_n$ are iid; it follows that
$$\limsup_{n \to \infty}\frac{X_n}{\log n} \geq \frac{1}{\lambda}$$
almost surely, which is what we needed to show.

To prove (\ref{Borel Cantelli 2}), we repeat the computation used to prove (\ref{Borel Cantelli 1}) to notice that
$$\PP\left(X_n > \frac{\log n}{\lambda} - \varepsilon \log n\right) = \exp(-(1-\varepsilon\lambda) \log n);$$
this time $-\varepsilon\lambda < 0$, so the sequence is not summable, and we conclude (\ref{Borel Cantelli 2}).

\begin{exer}
Suppose that $X_i \in L^1$ are iid random variables with expected value $\mu \neq 0$, $S_n = \sum_{j \leq n} X_j$, and $N$ is a geometric random variable with success probability $p$ which is independent of the $X_i$.
Show that $pS_N/\mu$ converges in distribution to an exponential random variable with some rate $\lambda$ as $p \to 0$, and compute $\lambda$.
\end{exer}

First,
$$|S_k(1-p)^k| = |\overline X_k| k(1-p)^k;$$
since $X_i \in L^1$, $\overline X_k$ converges to a Gaussian distribution in distribution, so $|\overline X_k|$ is almost surely uniformly bounded in $k$; therefore $|\overline X_k|\cdot k(1-p)^k$ is summable in $k$, and we may use Fubini's theorem to swap the order of summation and compute
\begin{align*}
S_N &= \sum_{k=1}^\infty \PP(N = k) S_k
= p\sum_{k=1}^\infty (1-p)^{k-1}(X_1 + \cdots + X_k)\\
&= p \sum_{j=1}^\infty X_j \sum_{k=j}^\infty (1-p)^{k-1}
= \sum_{j=1}^\infty (1-p)^{j-1} X_j.
\end{align*}
Writing $\hat Y$ for the characteristic function of the distribution of a random variable $Y$, it follows that, if $Y = pS_N/\mu$,
$$
\hat Y(\xi) = E\left(\exp\left(i\xi\frac{p}{\mu}\sum_{j=1}^\infty X_j\right)\right)
= \prod_{j=1}^\infty E(e^{i\xi p(1-p)^{j-1}X_1/\mu})
$$
since the $X_j$ are iid. That is,
$$\hat Y(\xi) = \prod_{j=1}^\infty \widehat{\frac{X_1}{\mu}}(p(1-p)^{j-1}\xi).$$
Since $E(X_1/\mu) = 1$, we get a Taylor expansion
$$\widehat{\frac{X_1}{\mu}}(p(1-p)^{j-1}) = 1 + ip(1-p)^{j-1}\xi + O(p^2(1-p)^{2(j-1)}|\xi|^2)$$
whenever $p$ is so small (depending on $\xi$) that $p^2 < (1-p)^{2(1-j)}|\xi|^2 \leq |\xi|^2$; thus, this occurs whenever $p < |\xi|$.
Moreover
$$\sum_{j=1}^\infty i\xi p(1-p)^{j-1} = i\xi$$
so to first order, $\hat Y(\xi) = 1 + i\xi$... TODO

\begin{exer}
Let $X_i$ be iid uniform signs, $S_n = \sum_{i \leq n} X_i$. Show that for every $\varepsilon > 0$,
$$\PP(S_n \geq n\varepsilon) \leq e^{-n\varepsilon^2/2}.$$
\end{exer}

Let $\overline X_n = S_n/n$ be the mean of the signs, with distribution $\mu_n$.
Let $M$ be the moment-generating function of $X_1$, so that
$$M(\theta) = \frac{1}{2}(e^\theta + e^{-\theta}).$$
A standard lemma to the proof of Cram\'er's theorem says that for every $\theta > 0$,
$$\mu_n([\varepsilon, \infty)) \leq e^{-n\varepsilon\theta} M(\theta)^n.$$
On the other hand, $M(\theta)^n = (\cosh \theta)^n$ so
$$\mu_n([n, \infty)) \leq (e^{-y\theta} \cosh \theta)^n.$$
We now claim that
\begin{equation}
\label{gaussian bound}
\inf_\theta e^{-y\theta} \cosh \theta \leq e^{-y^2/2}.
\end{equation}
To see (\ref{gaussian bound}), we let $F_\theta(y) = e^{-y\theta} \cosh \theta - e^{-y^2/2}$. Then $F_\theta$ has two distinct zeroes $y_1(\theta) < y_2(\theta)$, since
\begin{equation}
\label{quadratic equation}
-y\theta + \log \cosh \theta = \frac{y^2}{2}
\end{equation}
iff $F_\theta(y) = 0$, and (\ref{quadratic equation}) is a quadratic equation with discriminant $\theta^2 + 2\log \cosh \theta \geq 0$ (since $\theta \in \RR$, thus $\cos \theta \geq 1$).
Moreover, $F_\theta(0) \geq 0$ and $F_\theta(y) > 0$ if $y$ is sufficiently large, so $F_\theta \leq 0$ on $[y_1(\theta), y_2(\theta)]$.
Solving (\ref{quadratic equation}), we see that
$$y_i(\theta) = \theta + (-1)^{i}\sqrt{\theta^2 + 2 \log \sech \theta}$$
so in particular $y_1(\theta) \geq 0$ and $y_2(\theta) \to \infty$ as $\theta \to \infty$.
That implies that for any $y$ there is a $\theta$ such that $y \in [y_1(\theta), y_2(\theta)]$, so that $F_\theta(y) \leq 0$.
Therefore (\ref{gaussian bound}) holds.
We conclude that
$$\mu_n([n, \infty)) \leq e^{-ny^2/2}$$
which is what we wanted to show.




\begin{exer}
Let $X_i$ be iid random variables with continuous distribution. Let $B_n = \bigcap_{j < n} \{X_n > X_j\}$ whenever $n \geq 2$, and $B_1 = 1$. Show that
$$\lim_{n = 1}^\infty \frac{1}{\log n} \sum_{j=1}^n 1_{B_j} = 1$$
almost surely.
\end{exer}



\end{document}
