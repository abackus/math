
% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------

\documentclass[10pt]{article}

\usepackage[margin=.7in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{enumitem}
\usepackage{tikz-cd}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{algorithm2e}
\usepackage{verse,stmaryrd}
\usepackage{fancyvrb}

% Number systems
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\PP}{\mathbb P}
\newcommand{\FF}{\mathbb F}
\newcommand{\DD}{\mathbb D}
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\ch}{\operatorname{ch}}
\newcommand{\Con}{\operatorname{Con}}
\newcommand{\coker}{\operatorname{coker}}
\newcommand{\CVect}{\CC\operatorname{-Vect}}
\newcommand{\Cantor}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\card}{\operatorname{card}}
\newcommand{\dbar}{\overline \partial}
\newcommand{\diam}{\operatorname{diam}}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\End}{\operatorname{End}}
\DeclareMathOperator*{\esssup}{ess\,sup}
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\Ind}{\operatorname{Ind}}
\newcommand{\Inn}{\operatorname{Inn}}
\newcommand{\interior}{\operatorname{int}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\mesh}{\operatorname{mesh}}
\newcommand{\LL}{\mathcal L_0}
\newcommand{\Leb}{\mathcal{L}_{\text{loc}}^2}
\newcommand{\Lip}{\operatorname{Lip}}
\newcommand{\ppGL}{\operatorname{PGL}}
\newcommand{\ppic}{\vspace{35mm}}
\newcommand{\ppset}{\mathcal{P}}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator*{\Res}{Res}
\newcommand{\Riem}{\mathcal{R}}
\newcommand{\RVect}{\RR\operatorname{-Vect}}
\newcommand{\Sch}{\mathcal{S}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\spn}{\operatorname{span}}
\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\TT}{\mathcal T}
\DeclareMathOperator{\tr}{tr}

% Calculus of variations
\DeclareMathOperator{\pp}{\mathbf p}
\DeclareMathOperator{\zz}{\mathbf z}
\DeclareMathOperator{\uu}{\mathbf u}
\DeclareMathOperator{\vv}{\mathbf v}
\DeclareMathOperator{\ww}{\mathbf w}

% Categories
\newcommand{\Ab}{\mathbf{Ab}}
\newcommand{\Cat}{\mathbf{Cat}}
\newcommand{\Group}{\mathbf{Group}}
\newcommand{\Module}{\mathbf{Module}}
\newcommand{\Set}{\mathbf{Set}}
\DeclareMathOperator{\Fun}{Fun}
\DeclareMathOperator{\Iso}{Iso}

% Complex analysis
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Logic
\renewcommand{\iff}{\leftrightarrow}
\newcommand{\Henkin}{\operatorname{Henk}}
\newcommand{\PA}{\mathbf{PA}}
\newcommand{\Var}{\operatorname{Var}}
\DeclareMathOperator{\proves}{\vdash}

% Group
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Out}{Out}

% Other symbols
\newcommand{\heart}{\ensuremath\heartsuit}
\newcommand{\club}{\ensuremath\clubsuit}

\DeclareMathOperator{\atanh}{atanh}

% Theorems
\theoremstyle{definition}
\newtheorem*{corollary}{Corollary}
\newtheorem*{falselemma}{Grader's ``Lemma"}
\newtheorem{exer}{Exercise}
\newtheorem{lemma}{Lemma}[exer]
\newtheorem{theorem}[lemma]{Theorem}

\def\Xint#1{\mathchoice
{\XXint\displaystyle\textstyle{#1}}%
{\XXint\textstyle\scriptstyle{#1}}%
{\XXint\scriptstyle\scriptscriptstyle{#1}}%
{\XXint\scriptscriptstyle\scriptscriptstyle{#1}}%
\!\int}
\def\XXint#1#2#3{{\setbox0=\hbox{$#1{#2#3}{\int}$ }
\vcenter{\hbox{$#2#3$ }}\kern-.6\wd0}}
\def\ddashint{\Xint=}
\def\dashint{\Xint-}

\usepackage[backend=bibtex,style=alphabetic,maxcitenames=50,maxnames=50]{biblatex}
\renewbibmacro{in:}{}
\DeclareFieldFormat{pages}{#1}

\begin{document}
\noindent
\large\textbf{Probability, HW 5} \hfill \textbf{Aidan Backus} \\

% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------\

Tainara Gobetti Borges gave me a hint on this homework.
I also received significant help from office hours.

\begin{exer}
Prove the strong law of large numbers without assuming that $X_1 \in L^1$.
\end{exer}

We have already proven the strong law of large numbers when $X_1 \in L^1$, and we reduce to that case.
After multiplying by a sign we may assume that $EX_1 = +\infty$.
Assume that the $X_i$ are iid, and let $X_i^M = X_i1_{X_i \leq M}$. Therefore $-\infty < EX_i^M \leq M$.

We first show that the $X_i^M$ are iid in $i$.
If $F$ is the cdf of $X_1$, and $F^M_i$ is the cdf of $X_i^M$, then $F_i^M(x) = F(x)$ if $x < 0$, $F_i^M(x) = F(x) + \PP(X_1 > M)$ if $0 < x < M$, and $F_i^M(x) = F_i^M(M)$ if $x > M$.
None of those cases depend on $i$ so $F^M_i = F^M_1$.
By a similar argument one checks independence.

Therefore by the strong law of large numbers for $L^1$, one has
$$\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n X_i \geq \lim_{n \to \infty} \frac{1}{n} \sum_{i = 1}^n X_i^M = EX_1^M.$$
However, by monotone convergence, $EX_1^M \to EX_1 = \infty$, which was desired.

\begin{exer}
Let $X_n$ be independent random variables such that for every $n$, $EX_n = 0$ and $\Var X_n = 1$. Show that for every $\varepsilon > 0$,
$$\lim_{n \to \infty} \frac{1}{\sqrt n (\log n)^{\varepsilon + 1/2}} \sum_{j=1}^n X_j = 0$$
almost surely.
\end{exer}

Let
$$Y_n = \frac{1}{\sqrt n (\log n)^{\varepsilon + 1/2}} \sum_{j=1}^n X_j.$$
For convenience we take the logarithms to be base $2$; this cleans up the notation a little but only affects estimates by a constant factor.
We must show that $Y_n \to 0$ almost surely, and we first check that at least a subsequence converges:
\begin{lemma}
$Y_{2^n} \to 0$ almost surely.
\end{lemma}
\begin{proof}
Let $\delta > 0$. We must show that
$$\sum_{n=1}^\infty \PP(|Y_{2^n}| > \delta) \lesssim_{\varepsilon,\delta} 1.$$
Indeed,
$$\PP(|Y_{2^n}| > \delta) \lesssim_\delta \Var Y_n = \left(\frac{1}{2^{n/2}n^{\varepsilon+1/2}}\right)^2 \sum_{j=1}^n \Var X_j = \frac{1}{2^n n^{2\varepsilon}} \in \ell^1$$
since the $X_j$ are independent.
\end{proof}
Now assume that $n$ is large and $2^n \leq j \leq 2^{n+1} - 1$. Then
$$|Y_j| \leq |Y_{2^n}| + |Y_j - Y_{2^n}| \leq |Y_{2^n}| + \bigvee_{i=2^n}^{2^{n+1} - 1} |Y_i - Y_{2^n}|$$
so we need to bound $\bigvee_{i=2^n}^{2^{n+1} - 1} |Y_i - Y_{2^n}|$, since the $|Y_{2^n}|$ term is already controlled by the lemma.
In fact, by the Kolmogorov maximal inequality,
$$\PP\left(\bigvee_{i=2^n}^{2^{n+1} - 1} |Y_i - Y_{2^n}| > \delta\right) \lesssim_\delta \frac{\Var(S_{2^{n+1}} - S_{2^n})}{(2^{n/2}n^{1/2+\varepsilon})^2} = \frac{2^n}{2^n n^{1+2\varepsilon}} = \frac{1}{n^{1+2\varepsilon}} \in \ell^1$$
which gives the claim.

\begin{exer}
Let $X_i$ be independent random variables with $\PP(X_1 = \pm 1) = 1/2$, $\PP(X_n = \pm 1) = (1 - n^{-2})/2$, $\PP(X_n = \pm n^3) = (2n^2)^{-1}$.
Show that for every $\delta > 0$,
$$\lim_{n \to \infty} \frac{\sum_{j \leq n} X_j}{(\sum_{j \leq n} X_j^2)^{1/2 + \delta}} = 0$$
almost surely.
\end{exer}

We first note that
$$\PP\left(\limsup_{n \to \infty} X_n^2 = n^6\right) = 0,$$
a consequence of the Borel-Cantelli lemma. Indeed,
$$\sum_{n=2}^\infty \PP(X_n^2 = n^6) = \sum_{n=2}^\infty \frac{1}{n^2} = \frac{\pi^2}{6} - 1 < \infty.$$
Therefore, there is almost surely an $N$ such that if $n \geq N$ then $X_n^2 = 1$.
Let $Y_n = X_n/|X_n|$; then if $n \geq N$, $Y_n = X_n$. So
$$\frac{\sum_{j=1}^n X_j}{(\sum_{j=1}^n X_j^2)^{1/2+\delta}} \sim \frac{\sum_{j=1}^n X_j}{(\sum_{j=1}^n Y_j^2)^{1/2 + \delta}} = \frac{1}{n^{1/2+\delta}} \sum_{j=1}^n X_j
\sim \frac{1}{n^{1/2+\delta}} \sum_{j=1}^n Y_j.$$
We now must show that
$$\lim_{n \to \infty} \frac{1}{n^{1/2+\delta}} \sum_{j=1}^n Y_j = 0$$
almost surely.

We are in the setting of Kronecker's lemma, since $n^{1/2+\delta}$ is an increasing sequence; so it suffices to show that
$$\sum_{n=1}^\infty \frac{Y_n}{n^{1/2+\delta}} < \infty.$$
Since
$$\sum_{n=1}^\infty \Var \frac{Y_n}{n^{1/2+\delta}} = \sum_{n=1}^\infty \frac{1}{n^{1+2\delta}} < \infty$$
the Khintchine-Kolmogorov convergence theorem implies the claim.

\begin{exer}
Let $X_i$ be iid with
$$\PP(X_1 = n) = \PP(X_1 = -n) = \frac{c}{n^2 \log n}$$
with $c$ a normalization. Let $S_n = \sum_{j \leq n} X_j$.

Show that $\PP(|X_1| > n) \sim 1/(n \log n)$ and $E|X_1| = \infty$.

Show that $S_n/n \to 0$ in $\PP$.

Show that $\PP(\limsup_n |X_n| > n) = 1$. Show that $\PP(\limsup_n |S_n| > n/2) = 1$. Show that $\PP(S_n/n \to 0) = 0$.

Show that
$$\limsup_{n \to \infty} \frac{S_n}{n} = \infty$$
and that the $\liminf$ is $-\infty$ almost surely.
\end{exer}

\begin{theorem}
One has $\PP(|X_1| > n) \sim 1/(n \log n)$ and $E|X_1| = \infty$.
\end{theorem}
\begin{proof}
If $3 \leq j \leq n$ and $n$ is large enough then
$$\frac{1}{\log j} - \frac{1}{\log n} \leq \frac{1}{\log 3} - \frac{1}{\log n} \leq 2.$$
Therefore, with the same hypotheses on $n$ and $j$,
$$\frac{1}{j^2 \log n} \leq \frac{1}{j^2 \log j} = \frac{1}{j^2 \log n} + \frac{1}{j^2} \left(\frac{1}{\log j} - \frac{1}{\log n}\right) \leq \frac{2}{j^2 \log n}.$$
So $1/(j^2 \log j) \sim 1/(j^2 \log n)$. On the other hand,
$$\sum_{j=n+1}^\infty \frac{1}{j^2} \sim \int_{n+1}^\infty \frac{dx}{x^2} = \frac{1}{n + 1}.$$
Therefore
$$\PP(|X_1| > n) \sim \sum_{j=n+1}^\infty \frac{1}{j^2 \log j} \sim \frac{1}{\log n} \sum_{j=n+1}^\infty \frac{1}{j^2} \sim \frac{1}{n \log n}.$$
In particular,
$$E|X_1| + 1 \geq \sum_{n=3}^\infty \PP(|X_1| > n) \sim \sum_{n=3}^\infty \frac{1}{n \log n} \geq \frac{1}{2} \sum_{n=3}^\infty \frac{1}{n} = \infty.$$
So $E|X_1| = \infty$.
\end{proof}

\begin{theorem}
One has $S_n/n \to 0$ in $\PP$.
\end{theorem}
\begin{proof}
Let $Y_n = X_n 1_{|X_n| \leq n}$, and $Z_n = \sum_{j=1}^n Y_j$. Then
$$\PP(|S_n| > n\varepsilon) = \PP\left(|S_n| > n\varepsilon \wedge \bigwedge_{j=1}^n X_j = Y_j\right) + \PP\left(|S_n| > n\varepsilon \wedge \bigvee_{j=1}^n X_j \neq Y_j\right).$$
We bound
\begin{align*}
\PP\left(|S_n| > n\varepsilon \wedge \bigvee_{j=1}^n X_j \neq Y_j\right) &\leq \PP\left(\bigvee_{j=1}^n X_j \neq Y_j\right)\\
&\leq \sum_{j=1}^n \PP(X_j \neq Y_j) = n\PP(X_1 \neq Y_1)\\
&\sim \frac{n}{n \log n} = \frac{1}{\log n}.
\end{align*}
Moreover,
\begin{align*}
\PP\left(|S_n| > n\varepsilon \wedge \bigwedge_{j=1}^n X_j = Y_j\right) &= \PP\left(|Z_n| > n\varepsilon \wedge \bigwedge_{j=1}^n X_j = Y_j\right) \\
&\leq \PP(|Z_n| > n\varepsilon) \leq \frac{\Var Z_n}{n^2 \varepsilon^2} = \frac{\Var Y_1}{n\varepsilon^2} \\
&\sim \frac{1}{n\varepsilon^2} \sum_{k=3}^n \frac{k^2}{k^2 \log k} = \frac{1}{n\varepsilon^2} \sum_{k=3}^n \frac{1}{ \log k} \\
&\sim \frac{\log n}{\varepsilon^2}
\end{align*}
since the mean of $\log 3, \dots, \log n$ is $\sim \log n$. Therefore
$$\PP(|S_n| > n\varepsilon) \lesssim \frac{1}{\varepsilon^2\log n}$$
if $n$ is large enough and $\varepsilon$ is small enough.
Taking $n \to \infty$ we see the claim.
\end{proof}

\begin{lemma}
One has $\PP(\limsup_n |X_n| > n) = 1$.
\end{lemma}
\begin{proof}
We use the converse to the Borel-Cantelli lemma.
Since the events $|X_n| > n$ are independent by hypothesis, we just have to show that $\PP(|X_n| > n) \notin \ell^1$. But this just says
$$\PP(|X_n| > n) = \PP(|X_1| > n) \sim \frac{1}{n \log n}$$
by a previous theorem, which is not in $\ell^1$.
\end{proof}

\begin{lemma}
One has $\PP(\limsup_n |S_n| > n/2) = 1$.
\end{lemma}
\begin{proof}
Assume not, thus on a tail, $|S_n| \leq n/2$.
By the previous lemma, we can, almost surely, find arbitrarily large $k$ such that $|X_{k+1}| > k + 1$. One has $|S_k| \leq k/2$, so
$$|S_{k+1}| = |S_k + X_{k+1}| > k + 1 - \frac{k}{2} = \frac{k+1}{2},$$
a contradiction.
\end{proof}

\begin{theorem}
One has $\PP(S_n/n \to 0) = 0$.
\end{theorem}
\begin{proof}
By the previous lemma, it is almost surely true that $|S_n|/n > 1/2$ on a tail.
\end{proof}

\begin{lemma}
One has
$$\limsup_{n \to \infty} \frac{|S_n|}{n} = \infty$$
almost surely.
\end{lemma}
\begin{proof}
Suppose not. Since the $X_i$ are iid, the zero-one law implies that
$$\PP\left(\limsup_{n \to \infty} \frac{|S_n|}{n} = \infty\right) = 0$$
so there is almost surely a constant $C > 0$ such that $|S_n| \leq Cn$.
On the other hand, there is a $\delta > 0$ such that
$$\PP(|S_n| \geq \delta n^2) \geq \prod_{j=3}^n \PP(X_j = j) = \prod_{j=3}^n \frac{1}{j^2 \log j} \geq \frac{1}{n^{2n} \log n^2} > 0.$$
In particular, taking $n$ lare enough contradicts the existence of $C$.
\end{proof}

\begin{theorem}
One has
$$\limsup_{n \to \infty} \frac{S_n}{n} = -\liminf_{n \to \infty} \frac{S_n}{n} = \infty$$
almost surely.
\end{theorem}
\begin{proof}
By the previous lemma,
$$1 = \PP\left(\limsup_{n \to \infty} \frac{|S_n|}{n} = \infty\right) \geq \PP\left(\limsup_{n \to \infty} \frac{S_n}{n} = \infty\right) + \PP\left(\liminf_{n \to \infty} \frac{S_n}{n} = -\infty\right).$$
Therefore
$$\PP\left(\limsup_{n \to \infty} \frac{S_n}{n} = \infty\right) \geq \frac{1}{2}$$
by symmetry, so by the zero-one law, the claim holds for the $\limsup$.
The claim then holds for the $\liminf$ by symmetry again.
\end{proof}





\end{document}
