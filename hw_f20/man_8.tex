
% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------

\documentclass[10pt]{article}

\usepackage[margin=.7in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{enumitem}
\usepackage{tikz-cd}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{algorithm2e}
\usepackage{verse,stmaryrd}
\usepackage{fancyvrb}

% Number systems
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\PP}{\mathbb P}
\newcommand{\FF}{\mathbb F}
\newcommand{\DD}{\mathbb D}
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\cl}{\operatorname{cl}}
\newcommand{\ch}{\operatorname{ch}}
\newcommand{\Con}{\operatorname{Con}}
\newcommand{\coker}{\operatorname{coker}}
\newcommand{\CVect}{\CC\operatorname{-Vect}}
\newcommand{\Cantor}{\mathcal{C}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\card}{\operatorname{card}}
\newcommand{\dbar}{\overline \partial}
\newcommand{\diam}{\operatorname{diam}}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\End}{\operatorname{End}}
\DeclareMathOperator*{\esssup}{ess\,sup}
\newcommand{\Hess}{\operatorname{Hess}}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\Ind}{\operatorname{Ind}}
\newcommand{\Inn}{\operatorname{Inn}}
\newcommand{\interior}{\operatorname{int}}
\newcommand{\lcm}{\operatorname{lcm}}
\newcommand{\mesh}{\operatorname{mesh}}
\newcommand{\LL}{\mathcal L_0}
\newcommand{\Leb}{\mathcal{L}_{\text{loc}}^2}
\newcommand{\Lip}{\operatorname{Lip}}
\newcommand{\ppic}{\vspace{35mm}}
\newcommand{\ppset}{\mathcal{P}}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator*{\Res}{Res}
\newcommand{\Riem}{\mathcal{R}}
\newcommand{\RVect}{\RR\operatorname{-Vect}}
\newcommand{\Sch}{\mathcal{S}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\spn}{\operatorname{span}}
\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\TT}{\mathcal T}
\DeclareMathOperator{\tr}{tr}

% Calculus of variations
\DeclareMathOperator{\pp}{\mathbf p}
\DeclareMathOperator{\zz}{\mathbf z}
\DeclareMathOperator{\uu}{\mathbf u}
\DeclareMathOperator{\vv}{\mathbf v}
\DeclareMathOperator{\ww}{\mathbf w}

% Categories
\newcommand{\Ab}{\mathbf{Ab}}
\newcommand{\Cat}{\mathbf{Cat}}
\newcommand{\Group}{\mathbf{Group}}
\newcommand{\Module}{\mathbf{Module}}
\newcommand{\Set}{\mathbf{Set}}
\DeclareMathOperator{\Fun}{Fun}
\DeclareMathOperator{\Iso}{Iso}

% Complex analysis
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}

% Logic
\renewcommand{\iff}{\leftrightarrow}
\newcommand{\Henkin}{\operatorname{Henk}}
\newcommand{\PA}{\mathbf{PA}}
\DeclareMathOperator{\proves}{\vdash}

% Group
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Lie}{Lie}
\DeclareMathOperator{\Out}{Out}

\DeclareMathOperator{\Diffeo}{Diffeo}

\newcommand{\GL}{\operatorname{GL}}
\newcommand{\ppGL}{\operatorname{PGL}}
\newcommand{\SL}{\operatorname{SL}}
\newcommand{\SO}{\operatorname{SO}}
\newcommand{\iprod}{\mathbin{\lrcorner}}


% Other symbols
\newcommand{\heart}{\ensuremath\heartsuit}
\newcommand{\club}{\ensuremath\clubsuit}

\DeclareMathOperator{\atanh}{atanh}

% Theorems
\theoremstyle{definition}
\newtheorem*{corollary}{Corollary}
\newtheorem*{falselemma}{Grader's ``Lemma"}
\newtheorem{exer}{Exercise}
\newtheorem{lemma}{Lemma}[exer]
\newtheorem{theorem}[lemma]{Theorem}

\def\Xint#1{\mathchoice
{\XXint\displaystyle\textstyle{#1}}%
{\XXint\textstyle\scriptstyle{#1}}%
{\XXint\scriptstyle\scriptscriptstyle{#1}}%
{\XXint\scriptscriptstyle\scriptscriptstyle{#1}}%
\!\int}
\def\XXint#1#2#3{{\setbox0=\hbox{$#1{#2#3}{\int}$ }
\vcenter{\hbox{$#2#3$ }}\kern-.6\wd0}}
\def\ddashint{\Xint=}
\def\dashint{\Xint-}

\usepackage[backend=bibtex,style=alphabetic,maxcitenames=50,maxnames=50]{biblatex}
\renewbibmacro{in:}{}
\DeclareFieldFormat{pages}{#1}

\begin{document}
\noindent
\large\textbf{Manifolds, HW 8} \hfill \textbf{Aidan Backus} \\

% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------\

\begin{exer}[13.6]
Let $M$ be a Riemannian manifold; show that if $\dim M = 1$ then $M$ is flat.
\end{exer}

Let $x \in M$; then there is a small neighborhood $U$ around $x$ which is open and connected.
Since $U$ is $1$-dimensional we can put an ordering on it, for which the order topology is exactly the topology of $U$.
Let $\alpha < x < \beta$, $\alpha, \beta \in U$.
Then since $U$ is connected and thus diffeomorphic to $(-1, 2)$ (say), there is a strictly increasing curve $\gamma: [0, 1] \to U$ with $\gamma(0) = \alpha$ and $\gamma(1) = \beta$.
Then $\gamma$ has nonnzero velocity, so without loss of generality we may assume that $\gamma$ has unit speed.
In that case $\gamma$ is an isometry between $(0, 1)$ and $(\alpha, \beta)$, so $(\alpha, \beta)$ is flat.
Indeed, the Riemannian metric on $(0, 1)$ with coordinate function $x$ is $dx \otimes dx$, and if $y$ is the coordinate function on $(\alpha, \beta)$, then
$$\gamma_* dx \otimes dx = \gamma_* dx \otimes \gamma_* dx = \sigma dy \otimes \sigma dy = dy \otimes dy$$
where $\sigma$ is the sign of $\gamma'$.
Therefore $M$ is locally flat, hence flat.

\begin{exer}[13.13]
Let $M$ be a Riemannian manifold.
Show that the set of Killing fields on $M$ is a Lie algebra.
Show that a smooth vector field $V$ is Killing iff, in any coordinate chart,
$$V^k \frac{\partial g_{ij}}{\partial x^k} + g_{jk}\frac{\partial V^k}{\partial x^i} + g_{ik}\frac{\partial V^k}{\partial x^j} = 0.$$
\end{exer}

First, $V$ is Killing iff $\mathcal L_Vg = 0$.
By linearity it follows that the space of Killing fields is a vector space, and since
$$\mathcal L_{[X, Y]}g = [\mathcal L_X, \mathcal L_Y]g = 0$$
whenever $X$ and $Y$ are Killing,
that the commutator of two Killing fields is Killing; thus the Killing fields form a Lie algebra.

Expanding out the Lie derivative we see that for any vector fields $X,Y$,
$$\mathcal L_Vg(X, Y) = V(g(X, Y)) - g([V, X], Y) - g([V, Y], X).$$
Setting $V = V^k \partial_k$, $X = \delta^{i\mu} \partial_i$ and $Y = \delta^{j\nu} \partial_j$, one has
$$V(g(X, Y)) = V^k \partial_k(g_{\mu\nu} \delta^{i\mu} \delta^{j\nu} \partial_i \partial_j) = V^k \partial_k g_{ij}.$$
Now $[V, X] = -XV$ since $X$ is constant; similarly for $Y$, thus
$$- g([V, X], Y) - g([V, Y], X) = g(XV, Y) + g(X, YV).$$
In coordinates,
$$g(XV, Y) = g_{\mu\nu}(\delta^{k\mu} \partial_i V^k)\delta^{j\nu} = g_{kj}(\partial_i V^k).$$
Similarly for $Y$, hence the claim.

\begin{exer}[13.14]
Let $K$ denote the Lie algebra of Killing fields on $\RR^n$.
Let $K_0$ denote the subspace of Killing fields that vanish at $0$.
Show that
$$V \mapsto \left(\frac{\partial V^i}{\partial x^j}(0)\right)_{ij}$$
is an injective linear map $K_0 \to \mathfrak o(n)$.
Show that the vector fields $\partial_i$ and
$$x^i\frac{\partial}{\partial x^j} - x^j \frac{\partial}{\partial x^i}$$
form a basis for $K$.
\end{exer}

By the previous exercise, $V$ is Killing iff
$$0 = V^k \partial_k \delta_{ij} + \delta_{jk} \partial_i V^k + \partial_{ik} \partial_j V^k = \partial_i V^j + \partial_j V^i.$$
This implies the claim about the basis of $K$.
In addition, the total derivative
$$D_V = \left(\frac{\partial V^i}{\partial x^j}(0)\right)_{ij}$$
is skew-symmetric, hence $D_V \in \mathfrak o(n)$.
To see that the map $V \mapsto D_V$ is injective on $K_0$, assume $D_V = 0$.
Since $V$ is Killing and the metric is the Kronecker delta, $d(\theta_t)_0$ is constant in $t$.
So $V$ not only has a double zero at $0$, but everywhere, so $V = 0$.

\begin{exer}[14.4]
Show that $v_1, \dots, v_k$ are linearly dependent iff $\bigwedge_{j=1}^k v_j = 0$.
Show that the linearly independent tuples $v_1, \dots, v_k$ and $w_1, \dots, w_k$ have the same span iff there is a $c \neq 0$ such that $\bigwedge_{j=1}^k v_j = c\bigwedge_{j=1}^k w_j$.
\end{exer}

For the first claim, for every $i$, the $i$th statement in the below list is clearly equivalent to the $i+1$th:
\begin{enumerate}
\item $v_1, \dots, v_k$ are linearly dependent.
\item There are $\ell \leq k$ and $c_j$ such that $v_\ell = \sum_{j<\ell} c_j v_j$.
\item There are $\ell \leq k$ and $c_j$ such that
$$\bigotimes_j v_j = \sum_{m<\ell} c_m \bigotimes_{j<\ell} v_j \otimes v_m \otimes \bigotimes_{j>\ell} v_j.$$
\item $\bigotimes_j v_j$ is a linear combination of pure tensors with repeated entries.
\item $\bigwedge_j v_j = 0$.
\end{enumerate}
For the second claim, we reason similarly:
\begin{enumerate}
\item $v_1, \dots, v_k$ and $w_1, \dots, w_k$ have the same span.
\item There is a vector subspace $W$ such that $v_1, \dots, v_k$ is a basis of $W$ and $w_1, \dots, w_k$ is a basis of $W$.
\item For every $i$ there are $c_{ij}$ such that $w_i = \sum_j c_{ij} v_j$.
\item For every $i$ there are $c_{ij}$ such that
$$\bigwedge_i w_i = \sum_j \bigwedge_i c_{ij} v_j.$$
\item For every $i$ there are $c_{ij}$ such that
$$\bigwedge_i w_i = \sum_i \bigwedge_j c_{ji} v_j.$$
\item For every $i$ there are $c_i$ such that
$$\bigwedge_i w_i = \sum_i c_i \bigwedge_j v_j.$$
\item There is a $c$ such that
$$\bigwedge_i w_i = c\bigwedge_j v_j.$$
\end{enumerate}
Here we used the multilinearity of the alternating product to set $c_i = \prod_j c_{ji}$.

\begin{exer}[14.5]
Let $M$ be a manifold with boundary, and $\omega^1, \dots, \omega^k$ be $1$-forms on an open set $U \subseteq M$ which are linearly independent at each point.
Suppose that $\alpha^1, \dots, \alpha^k$ are $1$-forms on $U$ with $\sum_{i=1}^k \alpha^i \wedge \omega^i = 0$.
Show that $\alpha^i$ is in the $C^\infty(U)$-span of $\omega^1, \dots, \omega^k$.
\end{exer}

We induct on $k$. The base case $k = 1$ is the content of a previous exercise pointwise, and hence everywhere since the coefficients $c_j$ that appear in the solution to Exercise 14.4 will vary smoothly as long as $\omega^i$ and $\alpha^i$ are $1$-forms.

Now suppose that $\beta^1, \dots, \beta^{k-1}$ are in the span of $\omega^1, \dots, \omega^{k-1}$ whenever $\sum_i \beta^i \wedge \omega^i = 0$.
By assumption, $\sum_{i<k} \alpha^i \wedge \omega^i = -\alpha^k \wedge \omega^k$.
So if $\alpha^k \wedge \omega^k = 0$ then we're done by induction. Otherwise,
$$\alpha^k \wedge \omega^k \in \spn(\alpha^1 \wedge \omega^1, \cdots, \alpha^{k-1} \wedge \omega^{k-1}).$$
Since $\alpha^k$ and $\omega^k$ must be pointwise linearly independent, it follows that
$$\alpha^k \in \spn(\alpha^1, \dots, \alpha^k, \omega^1, \dots, \omega^{k-1}) = \spn(\omega^1, \dots, \omega^{k-1})$$
where the equality of spans comes from induction.

\begin{exer}[14.9]
Suppose that $\pi: M \to N$ is a surjective submersion with connected fibers.
Say that $v \in T_pM$ is vertical if $d\pi_p(v) = 0$.
Suppose that $\omega$ is a $k$-form on $M$. Show that there is a $k$-form $\eta$ on $N$ with $\omega = \pi^*\eta$ iff for every $p \in M$ and every vertical tangent vector $v$ at $p$, $v \iprod \omega_p = 0$ and $v \iprod d\omega_p = 0$.
\end{exer}

If $\omega = \pi^*\eta$ then
$$v \iprod \omega_p = v \iprod \pi^*\eta_p = \pi^*(d\pi_p(v) \iprod \eta_p) = \pi^*(0 \iprod \eta_p) = 0.$$
Similarly
$$v \iprod d\omega_p = v \iprod \pi^*d\eta_p = \pi^*(d\pi_p(v) \iprod d\eta_p) = \pi^*(0 \iprod d\eta_p) = 0.$$

To prove the converse, we first reduce to the case that $\pi$ is a linear projection.
Assume that $\dim M = m$ and $\dim N = n$.
For every $y \in N$, by assumption on $\pi$, $\pi^{-1}(y)$ is a connected manifold of dimension $n - m$.
In a small neighborhood $U$ of $p \in M$ and $V = \pi(U)$, for every $y \in V$, $U \cap \pi^{-1}(y)$ is diffeomorphic to $W$, where $W$ is an open subset of $\RR^{m - n}$ which does not depend on $y$.
Therefore $U$ is a fiber bundle over $V$ with fibers $V$, so if $U,V$ can be chosen so that $U$ is a trivial bundle over $V$.
That is, $U$ is diffeomorphic to $V \times W$, with $\pi$ the projection map.
So without loss of generality we may assume that $M = \RR^m$, $N = \RR^n$, and $\pi$ is a linear projection.
In this case, $v$ is vertical iff $v = (0, v')$ where $0 \in \RR^n$ and $v' \in \RR^{n - m}$.

Let $X$ be the vertical field $X = (0, \dots, 0, 1, \dots, 1)$ where there are $n$ zeroes and $n - m$ ones.
Then if $Y$ is a vector field on $\RR^m$, we can define a vector field $Y^\heart$ on $\RR^n$ by setting $Y^\heart = (Y, X)$.
Then $\pi_*Y^\heart = Y$.
Given $\omega$ such that $\omega$ and $d\omega$ kill vertical vectors, define a $k$-linear map $(\RR^n)^k \to \RR$ by
$$\eta(Y_1, \dots, Y_k) = \omega(Y_1^\heart, \dots, Y_k^\heart).$$
We first claim that $\eta$ is a $k$-form. Indeed, if $A_k$ is the alternating group and $\sigma \in A_k$ then
$$\eta(Y_{\sigma(1)}, \dots, Y_{\sigma(k)}) = \omega(Y_{\sigma(1)}^\heart, \dots, Y_{\sigma(k)}^\heart) = 0$$
since $\omega$ is a $k$-form.
Moreover,
$$\pi^*\eta((Y_1, Z_1), \dots, (Y_k, Z_k)) = \eta(Y_1, \dots, Y_k)$$
for any vector fields $Z_j$ on $\RR^{n - m}$. The same is clearly true of $\omega$, so $\omega = \pi^*\eta$.



\end{document}
